# 人工智能概论

## 一、基本概念

### 1.1 什么是人工智能

人工智能的定义可以分为两部分，即“人工”和“智能”。

“人工”比较好理解，总的来说，“人工系统”就是通常意义下的人工系统。

“智能”，很难定义。这涉及到其它诸如意识（CONSCIOUSNESS）、自我（SELF）、思维（MIND）（包括无意识的思维（UNCONSCIOUS_MIND））等等问题。因此人工智能的研究往往涉及对人的智能本身的研究。其它关于动物或其它人造系统的智能也普遍被认为是人工智能相关的研究课题。

人工智能学科的基本思想和基本内容是人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。

人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。

人工智能与思维科学的关系是实践和理论的关系，人工智能是处于思维科学的技术应用层次，是它的一个应用分支。

### 1.2 范式的演化

范式的演化大概有四个阶段。

#### 第一阶段：经验

通过观察和记录进行“经验归纳”。

#### 第二阶段：理论

通过严格的推导进行“理论演算”，从而得到结论。

#### 第三阶段：计算仿真

通过利用电子计算机对科学实验进行模拟仿真。

#### 第四阶段：数据探索

通过收集数据，分析数据，探索新的规律。

### 1.3 神经网络简介

神经网络由基本的神经元组成，图就是一个神经元的数学/计算模型，便于我们用程序来实现。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/NeuranCell.png" ch="500" />

#### 数学模型

- 输入 input
- 权重 weights
- 偏移 bias
- 求和计算 sum
- 激活函数 activation

#### 小结

- 一个神经元可以有多个输入。
- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元。
- 一个神经元的 $w$ 的数量和输入的数量一致。
- 一个神经元只有一个 $b$。
- $w$ 和 $b$ 有人为的初始值，在训练过程中被不断修改。
- $A$ 可以等于 $Z$，即激活函数不是必须有的。
- 一层神经网络中的所有神经元的激活函数必须一致。

#### 训练流程

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/TrainFlow.png" />

#### 主要功能

- 回归（Regression）或者叫做拟合（Fitting
- 分类（Classification）

#### 激活函数

激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。

引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。

### 1.4 神经网络三大概念

三大概念是：反向传播，梯度下降，损失函数。这三个概念是前后紧密相连的。

#### 基本工作原理

1. 初始化；
2. 正向计算；
3. 损失函数为我们提供了计算损失的方法；
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6. Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。

#### 反向传播

反向传播的思想其实就是，对于每一个训练实例，将它传入神经网络，计算它的输出；然后测量网络的输出误差（即期望输出和实际输出之间的差异），并计算出上一个隐藏层中各神经元为该输出结果贡献了多少的误差；反复一直从后一层计算到前一层，直到算法到达初始的输入层为止。此反向传递过程有效地测量网络中所有连接权重的误差梯度，最后通过在每一个隐藏层中应用梯度下降算法来优化该层的参数（反向传播算法的名称也因此而来）。

#### 梯度下降

“梯度下降”包含了两层含义：

1. 梯度：函数当前位置的最快上升点；
2. 下降：与导数相反的方向，用数学语言描述就是那个减号。

亦即与上升相反的方向运动，就是下降。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd_concept.png" ch="500" />

三要素：

1. 当前点；
2. 方向；
3. 步长。

梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。

在三维空间内的梯度下降过程

|观察角度1|观察角度2|
|--|--|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable2.png">|

在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。

在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。

### 1.5 损失函数

损失函数（loss function）是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。

在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。

#### 作用

损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。

#### 使用方法

1. 用随机值初始化前向计算公式的参数；
2. 代入样本，计算输出的预测值；
3. 用损失函数计算预测值和标签值（真实值）的误差；
4. 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5. 进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。

#### 图像理解

二维函数图像理解：
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd2d.png" />
纵坐标是损失函数值，横坐标是变量。不断地改变变量的值，会造成损失函数值的上升或下降。而梯度下降算法会让我们沿着损失函数值下降的方向前进。

等高线图理解：
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd3d.png" />
坐标是一个变量 $w$，纵坐标是另一个变量 $b$。两个变量的组合形成的损失函数值，在图中对应处于等高线上的唯一的一个坐标点。$w,b$ 所有不同值的组合会形成一个损失函数值的矩阵，我们把矩阵中具有相同损失函数值的点连接起来，可以形成一个不规则椭圆，其圆心位置，是损失值为 $0$ 的位置，也是我们要逼近的目标。

#### 常用函数

- 均方差函数，主要用于回归

  均方误差（MSE）是最常用的回归损失函数。MSE是目标变量和预测值之间的平方距离之和。
  
  平均绝对误差（MAE）是用于回归模型的另一个损失函数。MAE是目标和预测变量之间的绝对差异的总和。

  指数损失是在原有的损失函数上套一层指数，在adaboost上使用的就是指数损失，在加性模型中指数损失的主要吸引点在于计算上的方便。
- 交叉熵函数，主要用于分类
  
  交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p,q$ 的差异，其中 $p$ 表示真实分布，$q$ 表示预测分布，那么 $H(p,q)$ 就称为交叉熵：

  $$H(p,q)=\sum_i p_i \cdot \ln {1 \over q_i} = - \sum_i p_i \ln q_i $$

  交叉熵可在神经网络中作为损失函数，$p$ 表示真实标记的分布，$q$ 则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量 $p$ 与 $q$ 的相似性。

  交叉熵函数常用于逻辑回归(logistic regression)，也就是分类(classification)。

### 总结

梯度下降是神经网络的基本学习方法，我们会用单变量和双变量两种方式说明，配以可视化的图解。再多的变量就无法用可视化方式来解释了，所以我们力求用简单的方式理解复杂的事物。

## 二、线性回归

### 2.1 单变量线性回归

#### 回归模型简介

回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。

最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：

$$Y=a+bX+\varepsilon $$

$X$ 是自变量，$Y$ 是因变量，$\varepsilon$ 是随机误差，$a$ 和 $b$ 是参数，在线性回归模型中，$a,b$ 是我们要通过算法学习出来的。

#### 回归模型概念

- 通常假定随机误差 $\varepsilon$ 的均值为 $0$，方差为$σ^2$（$σ^2>0$，$σ^2$ 与 $X$ 的值无关）
- 若进一步假定随机误差遵从正态分布，就叫做正态线性模型
- 一般地，若有 $k$ 个自变量和 $1$ 个因变量（即公式1中的 $Y$），则因变量的值分为两部分：一部分由自变量影响，即表示为它的函数，函数形式已知且含有未知参数；另一部分由其他的未考虑因素和随机性影响，即随机误差
- 当函数为参数未知的线性函数时，称为线性回归分析模型
- 当函数为参数未知的非线性函数时，称为非线性回归分析模型
- 当自变量个数大于 $1$ 时称为多元回归
- 当因变量个数大于 $1$ 时称为多重回归

### 2.2 最小二乘法

#### 简介

最小二乘法，也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最小二乘法来表达。

#### 特性

1、线性特性

  所谓线性特性，是指估计量分别是样本观测值的线性函数，亦即估计量和观测值的线性组合。

2、无偏性

  无偏性，是指参数估计量的期望值分别等于总体真实参数。

3、最小方差性

  最小方差性，是指估计量与用其它方法求得的估计量比较，其方差最小，即最佳。最小方差性又称有效性。

#### 均方差

均方差(MSE - mean squared error)是回归任务中常用的手段：
$$
J = \frac{1}{2m}\sum_{i=1}^m(z_i-y_i)^2 = \frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2 
$$

### 2.3 梯度下降法

#### 简介

梯度下降法（英语：Gradient descent）是一个一阶最优化算法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。

#### 损失函数（Loss Function）

均方误差：
$$loss_i(w,b) = \frac{1}{2} (z_i-y_i)^2 $$

与最小二乘法比较可以看到，梯度下降法和最小二乘法的模型及损失函数是相同的，都是一个线性模型加均方差损失函数，模型用于拟合，损失函数用于评估效果。

### 2.4 神经网络法

#### 定义神经网络

我们是首次尝试建立神经网络，先用一个最简单的单层单点神经元，如图所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/Setup.png" ch="500" />

#### 工作原理

用神经元的编程模型把梯度下降法包装了一下，这样就进入了神经网络的世界，从而可以有成熟的方法论可以解决更复杂的问题，比如多个神经元协同工作、多层神经网络的协同工作等等。

### 2.5 三种方法比较

#### 单样本随机梯度下降

样本访问示意图如图所示。
  
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/SingleSample-example.png" />

特点
  - 训练样本：每次使用一个样本数据进行一次训练，更新一次梯度，重复以上过程。
  - 优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
  - 缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。

#### 小批量样本梯度下降

样本访问示意图如图所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/MiniBatch-example.png" />

特点
  - 训练样本：选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度。
  - 优点：不受单样本噪声影响，训练速度较快。
  - 缺点：batch size的数值选择很关键，会影响训练结果。

#### 全批量样本梯度下降

样本访问示意图如图所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/FullBatch-example.png" />

特点
  - 训练样本：每次使用全部数据集进行一次训练，更新一次梯度，重复以上过程。
  - 优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
  - 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。

#### 三种方式的比较

||单样本|小批量|全批量|
|---|---|---|---|
|梯度下降过程图解|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/SingleSample-Trace.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/MiniBatch-Trace.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/FullBatch-Trace.png"/>|
|批大小|1|10|100|
|学习率|0.1|0.3|0.5|
|迭代次数|304|110|60|
|epoch|3|10|60|
|结果|w=2.003, b=2.990|w=2.006, b=2.997|w=1.993, b=2.998|

### 2.6 多变量线性回归

建立多元线性回归模型时，为了保证回归模型具有优良的解释能力和预测效果，应首先注意自变量的选择，其准则是：

1. 自变量对因变量必须有显著的影响，并呈密切的线性相关；
2. 自变量与因变量之间的线性相关必须是真实的，而不是形式上的；
3. 自变量之间应具有一定的互斥性，即自变量之间的相关程度不应高于自变量与因变量之因的相关程度；
4. 自变量应具有完整的统计数据，其预测值容易确定。

#### 解决方法

两种方法的比较

|方法|正规方程|梯度下降|
|---|-----|-----|
|原理|几次矩阵运算|多次迭代|
|特殊要求|$X^{\top}X$ 的逆矩阵存在|需要确定学习率|
|复杂度|$O(n^3)$|$O(n^2)$|
|适用样本数|$m \lt 10000$|$m \ge 10000$|

#### 正规方程法

对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。它可以解决下面这个公式描述的问题：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k $$

#### 神经网络法

如图所示的一层的神经网络，输入层为2或者更多，反正大于2了就没区别。这个一层的神经网络的特点是：

1. 没有中间层，只有输入项和输出层（输入项不算做一层）；
2. 输出层只有一个神经元；
3. 神经元有一个线性输出，不经过激活函数处理，即在下图中，经过 $\Sigma$ 求和得到 $Z$ 值之后，直接把 $Z$ 值输出。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/5/setup.png" ch="500" />

#### 样本特征数据标准化

数据标准化（Normalization），又可以叫做数据归一化。

将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。

标准化的常用方法

- Min-Max标准化（离差标准化），将数据映射到 $[0,1]$ 区间

$$x_{new}=\frac{x-x_{min}}{x_{max} - x_{min}} $$

- 平均值标准化，将数据映射到[-1,1]区间
   
$$x_{new} = \frac{x - \bar{x}}{x_{max} - x_{min}} $$
- 对数转换
$$x_{new}=\ln(x_i) }$$

- 反正切转换
$$x_{new}=\frac{2}{\pi}\arctan(x_i) $$

- Z-Score法
$$x_{new} = \frac{x_i - \bar{x}}{std} $$

- 中心化，平均值为0，无标准差要求
  
$$x_{new} = x_i - \bar{x} $$

- 比例法，要求数据全是正值

$$ x_{new} = \frac{x_k}{\sum_{i=1}^m{x_i}} $$

### 总结

1. 样本不做标准化的话，网络发散，训练无法进行；
2. 训练样本标准化后，网络训练可以得到结果，但是预测结果有问题；
3. 还原参数值后，预测结果正确，但是此还原方法并不能普遍适用；
4. 标准化测试样本，而不需要还原参数值，可以保证普遍适用；
5. 标准化标签值，可以使得网络训练收敛快，但是在预测时需要把结果反标准化，以便得到真实值。

## 三、线性分类

### 3.1 线性二分类

线性二分类与非线性二分类的区别

|线性二分类|非线性二分类|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_binary.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/non_linear_binary.png"/>|

#### 二分类函数

对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。

根据不同的任务来区分激活函数和分类函数这两个概念，在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

- Logistic函数公式

$$Logistic(z) = \frac{1}{1 + e^{-z}}$$

#### 优点

- 把线性回归的成功经验引入到分类问题中，相当于对“分界线”的预测进行建模，而“分界线”在二维空间上是一条直线，这就不需要考虑具体样本的分布，避免了假设分布不准确所带来的问题；
- 不仅预测出类别（0/1），而且得到了近似的概率值（比如0.31或0.86），这对许多需要利用概率辅助决策的任务很有用；
- 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解。

#### 工作原理

线性回归和线性分类的比较

||线性回归|线性分类|
|---|---|---|
|相同点|需要在样本群中找到一条直线|需要在样本群中找到一条直线|
|不同点|用直线来拟合所有样本，使得各个样本到这条直线的距离尽可能最短|用直线来分割所有样本，使得正例样本和负例样本尽可能分布在直线两侧|

#### 总结

二分类：表示分类任务中有两个类别。也就是说，训练一个分类器，输入一幅图片，用特征向量x表示，输出是不是猫，用y=0或1表示。二类分类是假设每个样本都被设置了一个且仅有一个标签 0 或者 1。

### 3.2 线性多分类

#### 和非线性的区别

线性多分类和非线性多分类的区别在于不同类别的样本点之间是否可以用一条直线来互相分割。对神经网络来说，线性多分类可以使用单层结构来解决，而分线性多分类需要使用双层结构。

#### 多分类函数

- Logistic函数公式

$$ a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} $$

#### 函数效果

把三张图综合在一起，应该是。
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/z123.png" ch="500" />

### 总结

神经网络的一个重要功能就是分类，现实世界中的分类任务复杂多样，但万变不离其宗，我们都可以用同一种模式的神经网络来处理。

## 四、非线性回归

### 4.1 激活函数

激活函数用在神经网络的层与层之间的连接，神经网络的最后一层不用激活函数。

#### 作用

1. 给神经网络增加非线性因素，这个问题在第1章神经网络基本工作原理中已经讲过了；
2. 把公式中的计算结果压缩到 $[0,1]$ 之间，便于后面的计算。

#### 基本性质

- 非线性：线性的激活函数和没有激活函数一样；
- 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性；
- 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出。

#### 挤压型

通常用Sigmoid来表示，原意是S型的曲线，在数学中是指一类具有压缩作用的S型的函数，在神经网络中，有两个常用的Sigmoid函数，一个是Logistic函数，另一个是Tanh函数。

- Logistic函数
  
  公式：
  $$Sigmoid(z) = \frac{1}{1 + e^{-z}} \rightarrow a $$

  导数：
  $$Sigmoid'(z) = a(1 - a) $$

  值域：
  - 输入值域：$(-\infty, \infty)$
  - 输出值域：$(0,1)$
  - 导数值域：$(0,0.25]$

  函数图像：
  <img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/sigmoid.png" ch="500" />

  优点：
  - 从函数图像来看，Sigmoid函数的作用是将输入压缩到 $(0,1)$ 这个区间范围内，这种输出在0~1之间的函数可以用来模拟一些概率分布的情况。它还是一个连续函数，导数简单易求。  
  - 从数学上来看，Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。 
  - 从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。
  
  缺点：
  指数计算代价大。

- Tanh函数
  
  公式：
  $$Tanh(z) = 2 \cdot Sigmoid(2z) - 1 $$

  导数公式：
  $$Tanh'(z) = (1 + a)(1 - a)$$

  值域
  - 输入值域：$(-\infty,\infty)$
  - 输出值域：$(-1,1)$
  - 导数值域：$(0,1)$
  
  函数图像是双曲正切的函数图像。
  <img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/tanh.png" ch="500" />

  优点：具有Sigmoid的所有优点。

  缺点：exp指数计算代价大。梯度消失问题仍然存在。

### 总结

1. Sigmoid，指的是对数几率函数用于激活函数时的称呼；
2. Logistic，指的是对数几率函数用于二分类函数时的称呼；
3. Tanh，指的是双曲正切函数用于激活函数时的称呼。

#### 半线型

又可以叫非饱和型激活函数。

- ReLU函数 
  Rectified Linear Unit，修正线性单元，线性整流函数，斜坡函数。

  公式
  $$ReLU(z) = max(0,z) = \begin{cases} 
  z, & z \geq 0 \\\\ 
  0, & z < 0 
  \end{cases}$$

  优点
  - 反向导数恒等于1，更加有效率的反向传播梯度值，收敛速度快；
  - 避免梯度消失问题；
  - 计算简单，速度快；
  - 活跃度的分散性使得神经网络的整体计算成本下降。
  
  缺点：无界。梯度很大的时候可能导致的神经元“死”掉。

- Leaky ReLU函数
  LReLU，带泄露的线性整流函数。

  公式
  $$LReLU(z) = \begin{cases} z & z \geq 0 \\\\ \alpha \cdot z & z < 0 \end{cases}$$

  优点：继承了ReLU函数的优点。

- Softplus函数
  公式
  $$Softplus(z) = \ln (1 + e^z)$$

- ELU函数
  公式
  $$ELU(z) = \begin{cases} z & z \geq 0 \\ \alpha (e^z-1) & z < 0 \end{cases}$$

### 4.2 非线性回归

#### 评估标准

- 平均绝对误差

  MAE（Mean Abolute Error）。
  $$MAE=\frac{1}{m} \sum_{i=1}^m \lvert a_i-y_i \rvert $$

- 绝对平均值率误差

  MAPE（Mean Absolute Percentage Error）。
  $$MAPE=\frac{100}{m} \sum^m_{i=1} \left\lvert {a_i - y_i \over y_i} \right\rvert $$

- 和方差

  SSE（Sum Squared Error）。
  $$SSE=\sum_{i=1}^m (a_i-y_i)^2 $$

- 均方差

  MSE（Mean Squared Error）。
  $$MSE = \frac{1}{m} \sum_{i=1}^m (a_i-y_i)^2 $$

- 均方根误差

  RMSE（Root Mean Squard Error）。
  $$RMSE = \sqrt{\frac{1}{m} \sum_{i=1}^m (a_i-y_i)^2} $$

### 总结

非线性回归是回归函数关于未知回归系数具有非线性结构的回归。常用的处理方法有回归函数的线性迭代法、分段回归法、迭代最小二乘法等。非线性回归分析的主要内容与线性回归分析相似。

## 五、非线性分类

### 5.1 非线性二分类

#### 评估标准

- 准确率 Accuracy
- 混淆矩阵
- 精确率/查准率 Precision
- 召回率/查全率 Recall
- TPR - True Positive Rate 真正例率
- 调和平均值 F1
- ROC曲线与AUC
- Kappa statics

#### 使用原因

- 从简单证明异或问题的不可能性；
- 从复杂程度上分，有线性/非线性之分；
- 从样本类别上分，有二分类/多分类之分。

#### 实现

1. 定义神经网络结构
2. 前向计算
3. 反向传播
   
#### 总结

- 2个神经元肯定是足够的；
- 4个神经元肯定要轻松一些，用的迭代次数最少。
- 而更多的神经元也并不是更轻松；
- 而16个神经元更是事倍功半地把4个样本分到了4个区域上，神经网络可以做更强大的事情。

### 5.2 非线性多分类

#### 定义神经网络结构

先设计出能完成非线性多分类的网络结构，如图所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/nn.png" />

#### 样本不平衡

样本不平衡（Imbalanced Data）指的是数据集中各个类别的样本数量极不均衡。

样本不平衡会使得我们的分类模型存在很严重的偏向性，但是从一些常用的指标上又无法看出来。

解决方法：
- 平衡数据集
- 尝试其它评价指标
- 尝试产生人工数据样本
- 尝试一个新的角度理解问题
- 修改现有算法
- 集成学习

### 5.3 多变量非线性多分类

#### 梯度检查

梯度检验（gradient check）。通过计算数值梯度，得到梯度的近似值，然后和反向传播得到的梯度进行比较，若两者相差很小的话则证明反向传播的代码是正确无误的。

#### 学习率与批大小

batch size和学习率的关系可以大致总结如下：

1. 增加batch size，需要增加学习率来适应，可以用线性缩放的规则，成比例放大
2. 到一定程度，学习率的增加会缩小，变成batch size的$\sqrt m$倍
3. 到了比较极端的程度，无论batch size再怎么增加，也不能增加学习率了

### 总结

线性扩展模型。即把非线性数据通过一个非线性转换，变为新的数据分布，并且这些数据符合线性分布。这样就可以使用线性方法进行分类处理。

## 六、深度神经网络

### 6.1 深度神经网络框架设计

#### 功能/模式分析

1，2，3三层的模式完全一样：矩阵运算+激活/分类函数。

#### 抽象与设计
- NeuralNet

  首先需要一个`NeuralNet`类，来包装基本的神经网络结构和功能：

  - `Layers` - 神经网络各层的容器，按添加顺序维护一个列表
  - `Parameters` - 基本参数，包括普通参数和超参
  - `Loss Function` - 提供计算损失函数值，存储历史记录并最后绘图的功能
  - `LayerManagement()` - 添加神经网络层
  - `ForwardCalculation()` - 调用各层的前向计算方法
  - `BackPropagation()` - 调用各层的反向传播方法
  - `PreUpdateWeights()` - 预更新各层的权重参数
  - `UpdateWeights()` - 更新各层的权重参数
  - `Train()` - 训练
  - `SaveWeights()` - 保存各层的权重参数
  - `LoadWeights()` - 加载各层的权重参数

- Layer

  是一个抽象类，以及更加需要增加的实际类，包括：

  - Fully Connected Layer
  - Classification Layer
  - Activator Layer
  - Dropout Layer
  - Batch Norm Layer

  将来还会包括：

  - Convolution Layer
  - Max Pool Layer

  每个Layer都包括以下基本方法：
  - `ForwardCalculation()` - 调用本层的前向计算方法
  - `BackPropagation()` - 调用本层的反向传播方法
  - `PreUpdateWeights()` - 预更新本层的权重参数
  - `UpdateWeights()` - 更新本层的权重参数
  - `SaveWeights()` - 保存本层的权重参数
  - `LoadWeights()` - 加载本层的权重参数

- Activator Layer

  激活函数和分类函数：

  - `Identity` - 直传函数，即没有激活处理
  - `Sigmoid`
  - `Tanh`
  - `Relu`

- Classification Layer

  分类函数，包括：

  - `Sigmoid`二分类
  - `Softmax`多分类


 - Parameters

   基本神经网络运行参数：

   - 学习率
   - 最大`epoch`
   - `batch size`
   - 损失函数定义
   - 初始化方法
   - 优化器类型
   - 停止条件
   - 正则类型和条件

- LossFunction

  损失函数及帮助方法：

  - 均方差函数
  - 交叉熵函数二分类
  - 交叉熵函数多分类
  - 记录损失函数
  - 显示损失函数历史记录
  - 获得最小函数值时的权重参数

- Optimizer

  优化器：

  - `SGD`
  - `Momentum`
  - `Nag`
  - `AdaGrad`
  - `AdaDelta`
  - `RMSProp`
  - `Adam`

- WeightsBias

  权重矩阵，仅供全连接层使用：

  - 初始化 
    - `Zero`, `Normal`, `MSRA` (`HE`), `Xavier`
    - 保存初始化值
    - 加载初始化值
  - `Pre_Update` - 预更新
  - `Update` - 更新
  - `Save` - 保存训练结果值
  - `Load` - 加载训练结果值

- DataReader

  样本数据读取器：

  - `ReadData` - 从文件中读取数据
  - `NormalizeX` - 归一化样本值
  - `NormalizeY` - 归一化标签值
  - `GetBatchSamples` - 获得批数据
  - `ToOneHot` - 标签值变成OneHot编码用于多分类
  - `ToZeroOne` - 标签值变成0/1编码用于二分类
  - `Shuffle` - 打乱样本顺序

  从中派生出两个数据读取器：

  - `MnistImageDataReader` - 读取MNIST数据
  - `CifarImageReader` - 读取Cifar10数据

#### 回归任务
这个模型很简单，一个双层的神经网络，第一层后面接一个Sigmoid激活函数，第二层直接输出拟合数据。

超参数说明：

1. 输入层1个神经元，因为只有一个`x`值
2. 隐层4个神经元，对于此问题来说应该是足够了，因为特征很少
3. 输出层1个神经元，因为是拟合任务
4. 学习率=0.5
5. 最大`epoch=10000`轮
6. 批量样本数=10
7. 拟合网络类型
8. Xavier初始化
9. 绝对损失停止条件=0.001

#### 二分类任务

是一个双层神经网络，但是最后一层要接一个Logistic二分类函数来完成二分类任务。

超参数说明：

1. 输入层神经元数为2
2. 隐层的神经元数为3，使用Sigmoid激活函数
3. 由于是二分类任务，所以输出层只有一个神经元，用Logistic做二分类函数
4. 最多训练1000轮
5. 批大小=5
6. 学习率=0.1
7. 绝对误差停止条件=0.02

#### 多分类任务
用Mini框架，使用Sigmoid做为激活函数的两层网络，

超参数说明

1. 隐层8个神经元
2. 最大`epoch=5000`
3. 批大小=10
4. 学习率0.1
5. 绝对误差停止条件=0.08
6. 多分类网络类型
7. 初始化方法为Xavier

### 6.2 网络优化
随着网络的加深，训练变得越来越困难，时间越来越长，原因可能是：

- 参数多
- 数据量大
- 梯度消失
- 损失函数坡度平缓

为了解决上面这些问题，科学家们在深入研究网络表现的前提下，发现在下面这些方向上经过一些努力，可以给深度网络的训练带来或多或少的改善：

- 权重矩阵初始化
- 批量归一化
- 梯度下降优化算法
- 自适应学习率算法

#### 零初始化

即把所有层的`W`值的初始值都设置为0。

$$
W = 0
$$

但是对于多层网络来说，绝对不能用零初始化，否则权重值不能学习到合理的结果。

#### 标准初始化

标准正态初始化方法保证激活函数的输入均值为0，方差为1。

#### Xavier初始化方法

基于上述观察，Xavier Glorot等人研究出了下面的Xavier初始化方法。

条件：正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变。

#### MSRA初始化方法

MSRA初始化方法，又叫做He方法，因为作者姓何。

条件：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变。

网络初始化是一件很重要的事情。但是，传统的固定方差的高斯分布初始化，在网络变深的时候使得模型很难收敛。

#### 小结

表 几种初始化方法的应用场景

|ID|网络深度|初始化方法|激活函数|说明|
|---|---|---|---|---|
|1|单层|零初始化|无|可以|
|2|双层|零初始化|Sigmoid|错误，不能进行正确的反向传播|
|3|双层|随机初始化|Sigmoid|可以|
|4|多层|随机初始化|Sigmoid|激活值分布成凹形，不利于反向传播|
|5|多层|Xavier初始化|Tanh|正确|
|6|多层|Xavier初始化|ReLU|激活值分布偏向0，不利于反向传播|
|7|多层|MSRA初始化|ReLU|正确|

从表中可以看到，由于网络深度和激活函数的变化，使得人们不断地研究新的初始化方法来适应，最终得到1、3、5、7这几种组合。

### 6.3 梯度下降优化算法

#### 随机梯度下降 SGD
- 输入和参数
  - $ \eta $ - 全局学习率

#### 动量算法 Momentum

SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定，因为数据有噪音。

Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。

- 输入和参数

  - $\eta$ - 全局学习率
  - $\alpha$ - 动量参数，一般取值为0.5, 0.9, 0.99
  - $v_t$ - 当前时刻的动量，初值为0

#### 梯度加速算法 NAG

Nesterov Accelerated Gradient，或者叫做Nesterov Momentum。

- 输入和参数

  - $\eta$ - 全局学习率
  - $\alpha$ - 动量参数，缺省取值0.9
  - $v$ - 动量，初始值为0

### 6.4 自适应学习率算法

#### AdaGrad

Adaptive subgradient method.

AdaGrad是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。

- 输入和参数

  - $\eta$ - 全局学习率
  - $\epsilon$ - 用于数值稳定的小常数，建议缺省值为`1e-6`
  - $r=0$ 初始值

#### AdaDelta

Adaptive Learning Rate Method. 

AdaDelta法是AdaGrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，AdaDelta法仅计算在一个大小为w的时间区间内梯度值的累积和。

但该方法并不会存储之前梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：关于过去梯度值的衰减均值，当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。

- 输入和参数

  - $\epsilon$ - 用于数值稳定的小常数，建议缺省值为1e-5
  - $\alpha \in [0,1)$ - 衰减速率，建议0.9
  - $s$ - 累积变量，初始值0
  - $r$ - 累积变量变化量，初始为0

#### 均方根反向传播 RMSProp

Root Mean Square Prop。

RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。RMSprop法要解决AdaGrad的学习率缩减问题。

- 输入和参数

  - $\eta$ - 全局学习率，建议设置为0.001
  - $\epsilon$ - 用于数值稳定的小常数，建议缺省值为1e-8
  - $\alpha$ - 衰减速率，建议缺省取值0.9
  - $r$ - 累积变量矩阵，与$\theta$尺寸相同，初始化为0

#### Adam - Adaptive Moment Estimation

计算每个参数的自适应学习率，相当于RMSProp + Momentum的效果，Adam$^{[4]}$算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。

- 输入和参数

  - $t$ - 当前迭代次数
  - $\eta$ - 全局学习率，建议缺省值为0.001
  - $\epsilon$ - 用于数值稳定的小常数，建议缺省值为1e-8
  - $\beta_1, \beta_2$ - 矩估计的指数衰减速率，$\in[0,1)$，建议缺省值分别为0.9和0.999

### 6.5 批量归一化的原理

#### 基本数学知识

- 正态分布

  正态分布，又叫做高斯分布。

  若随机变量$X$，服从一个位置参数为$\mu$、尺度参数为$\sigma$的概率分布，且其概率密度函数为：

  $$ f(x)=\frac{1}{\sigma\sqrt{2 \pi} } e^{- \frac{{(x-\mu)^2}}{2\sigma^2}} $$

  则这个随机变量就称为正态随机变量，正态随机变量服从的分布就称为正态分布，记作：
  $$ X \sim N(\mu,\sigma^2) $$

  当μ=0,σ=1时，称为标准正态分布：
  $$ X \sim N(0,1) $$

  此时公式简化为：
  $$ f(x)=\frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}} $$

#### 深度神经网络的挑战

机器学习领域有个很重要的假设：I.I.D.（独立同分布）假设，就是假设训练数据和测试数据是满足相同分布的，这样就能做到通过训练数据获得的模型能够在测试集获得好的效果。

#### 批量归一化

既然可以把原始训练样本做归一化，那么如果在深度神经网络的每一层，都可以有类似的手段，也就是说把层之间传递的数据移到0点附近，那么训练效果就应该会很理想。这就是批归一化BN的想法的来源。

深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢，这是个在DL领域很接近本质的问题。

#### 前向计算

- 符号表
  表中，m表示batch_size的大小，比如32或64个样本/批；n表示features数量，即样本特征值数量。

  表 各个参数的含义和数据形状 

  |符号|数据类型|数据形状|
  |:---------:|:-----------:|:---------:|
  |$X$| 输入数据矩阵 | [m, n] |
  |$x_i$|输入数据第i个样本| [1, n] |
  |$N$| 经过归一化的数据矩阵 | [m, n] |
  |$n_i$| 经过归一化的单样本 | [1, n] |
  |$\mu_B$| 批数据均值 | [1, n] |
  |$\sigma^2_B$| 批数据方差 | [1, n] |
  |$m$|批样本数量| [1] |
  |$\gamma$|线性变换参数| [1, n] |
  |$\beta$|线性变换参数| [1, n] |
  |$Z$|线性变换后的矩阵| [1, n] |
  |$z_i$|线性变换后的单样本| [1, n] |
  |$\delta$| 反向传入的误差 | [m, n] |

如无特殊说明，以下乘法为元素乘，即element wise的乘法。

#### 批量归一化的优点

1. 可以选择比较大的初始学习率，让你的训练速度提高。

    以前还需要慢慢调整学习率，甚至在网络训练到一定程度时，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；

2. 减少对初始化的依赖

    一个不太幸运的初始化，可能会造成网络训练实际很长，甚至不收敛。

3. 减少对正则的依赖

   在第16章中，我们将会学习正则化知识，以增强网络的泛化能力。采用BN算法后，我们会逐步减少对正则的依赖，比如令人头疼的dropout、L2正则项参数的选择问题，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；

### 6.6 正则化

#### 拟合程度比较
神经网络的两大功能：回归和分类。这两类任务，都会出现欠拟合和过拟合现象。

回归任务中的三种情况，依次为：欠拟合、正确的拟合、过拟合。

分类任务中的三种情况，依次为：分类欠妥、正确的分类、分类过度。由于分类可以看作是对分类边界的拟合，所以我们经常也统称其为拟合。

出现过拟合的原因：

1. 训练集的数量和模型的复杂度不匹配，样本数量级小于模型的参数
2. 训练集和测试集的特征分布不一致
3. 样本噪音大，使得神经网络学习到了噪音，正常样本的行为被抑制
4. 迭代次数过多，过分拟合了训练数据，包括噪音部分和一些非重要特征

#### 解决过拟合问题

有了直观感受和理论知识，下面我们看看如何解决过拟合问题：

1. 数据扩展
2. 正则
3. 丢弃法
4. 早停法
5. 集成学习法
6. 特征工程（属于传统机器学习范畴，不在此处讨论）
7. 简化模型，减小网络的宽度和深度

### 6.7 偏差与方差

#### 偏差-方差分解

除了用上面的试验来估计泛化误差外，我们还希望在理论上分析其必然性，这就是偏差-方差分解的作用，bias-variance decomposition。

表 符号含义

|符号|含义|
|---|---|
|$x$|测试样本|
|$D$|数据集|
|$y$|x的真实标记|
|$y_D$|x在数据集中标记(可能有误差)|
|$f$|从数据集D学习的模型|
|$f_{x;D}$|从数据集D学习的模型对x的预测输出|
|$f_x$|模型f对x的期望预测输出|

学习算法期望的预测：
$$f_x=E[f_{x;D}] $$

不同的训练集/验证集产生的预测方差：
$$var(x)=E[(f_{x;D}-f_x)^2] $$

噪声：
$$\epsilon^2=E[(y_D-y)^2] $$

期望输出与真实标记的偏差：
$$bias^2(x)=(f_x-y)^2 $$

所以，各个项的含义是：

- 偏差：度量了学习算法的期望与真实结果的偏离程度，即学习算法的拟合能力。
- 方差：训练集与验证集的差异造成的模型表现的差异。
- 噪声：当前数据集上任何算法所能到达的泛化误差的下线，即学习问题本身的难度。

一般来说，偏差与方差是有冲突的，称为偏差-方差窘境 (bias-variance dilemma)。

- 早停法 Early Stopping

早停法，实际上也是一种正则化的策略，可以理解为在网络训练不断逼近最优解的过程种（实际上这个最优解是过拟合的），在梯度等高线的外围就停止了训练，所以其原理上和L2正则是一样的，区别在于得到解的过程。

- 丢弃法 Dropout

### 6.8 数据增强 Data Augmentation

#### 图像数据增强

- 旋转

  定义图片中心和旋转角度，进行微小的旋转。

- 缩放

- 平移和添加噪音

#### 其它图像处理方法

- 翻转图像：即左右镜像，或者上下镜像，但是对于数字识别来说不合适
- 剪裁图像：从图像中随机选择一部分，再调整为原始图像大小，对于本例也不适合
- 颜色变化：对图像进行颜色抖动，即对RGB值进行随机扰动，如椒盐噪声和高斯噪声
- 对比度变化：通过修改HSV空间中的色调和饱和度来改变图像的对比度，也可以用直方图均衡化
- 亮度变化：改变整个图像的亮度
- 颜色增强：对于颜色暗淡的图片进行全图的颜色增强

#### 多样本合成法

- SMOTE

  SMOTE,Synthetic Minority Over-sampling Technique$^{[1]}$，通过人工合成新样本来处理样本不平衡问题，提升分类器性能。

  类不平衡现象是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，仅占总体的1%，所能提取的相应特征也极少，即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但在验证环节分类效果不佳。

  基于插值的SMOTE方法为小样本类合成新的样本，主要思路为：

  1. 定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定采样倍率N；
  2. 对每一个小样本类样本$(x,y)$，按欧氏距离找K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为$(x_n,y_n)$。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式:

  $$(x_{new},y_{new})=(x,y)+rand(0,1)\times ((x_n-x),(y_n-y))$$

  3. 重复选取取样，直到大、小样本数量平衡。

- SamplePairing

  SamplePairing$^{[2]}$方法的处理流程如图16-35所示，从训练集中随机抽取两张图片分别经过基础数据增强操作（如随机翻转等）处理后经像素取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。

- Mixup

  Mixup$^{[3]}$是基于邻域风险最小化（VRM）原则的数据增强方法，使用线性插值得到新样本数据。在邻域风险最小化原则下，根据特征向量线性插值将导致相关目标线性插值的先验知识，可得出简单且与数据无关的mixup公式：

  $$
  x_n=\lambda x_i + (1-\lambda)x_j \\\\
  y_n=\lambda y_i + (1-\lambda)y_j
  $$

  其中$(x_n，y_n)$是插值生成的新数据，$(x_i,y_i)$和$(x_j，y_j)$是训练集中随机选取的两个数据，λ的取值满足贝塔分布，取值范围介于0到1，超参数α控制特征目标之间的插值强度。

#### 小结

Mixup、SMOTE、SamplePairing三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，但所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。但在特征空间中，小样本数据的真实分布可能并不限于该区域中，在给定范围之外适当插值，也许能实现更好的数据增强效果。

### 6.9 集成学习 Ensemble Learning

#### 集成学习的概念

当数据集有问题，或者网络学习能力不足，或准确度不够时，我们可以采取集成学习的方法，来提升性能。说得通俗一些，就是发挥团队的智慧，根据团队中不同背景、不同能力的成员的独立意见，通过某种决策方法来解决一个问题。所以集成学习也称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)等。

- Individual Learner 个体学习器

  如果所有的个体学习器都是同一类型的学习器，即同质模式，比如都用神经网路，称为“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。

  在传统的机器学习中，个体学习器可以是不同的，比如用决策树、支持向量机等，此时称为异质模式。

- Aggregator 结合模块

  个体学习器的输出，通过一定的结合策略，在结合模块中有机结合在一起，可以形成一个能力较强的学习器，所以有时称为强学习器，而相应地称个体学习器为弱学习器。

  个体学习器之间是否存在依赖关系呢？这取决于产生个体学习器的方法：

  - Boosting系列算法，一系列的个体学习器需要一个个地串行生成，有前后依赖关系。
  - Bagging算法和随机森林算法（Random Forest），个体学习器可以独立或并行生成，没有依赖关系。

#### Bagging法集成学习的基本流程

Bagging集成学习示意图

1. 首先是数据集的使用，采用自助采样法（Bootstrap Sampling）。假设原始数据集Training Set中有1000个样本，我们从中随机取一个样本的拷贝放到Training Set-1中，此样本不会从原始数据集中被删除，原始数据集中还有1000个样本，而不是999个，这样下次再随机取样本时，此样本还有可能被再次选到。如此重复m次（此例m=1000），我们可以生成Training Set-1。一共重复N次（此例N=9），可以得到N个数据集。
2. 然后搭建一个神经网络模型，可以参数相同。在N个数据集上，训练出N个模型来。
3. 最后再进入Aggregator。N值不能太小，否则无法提供差异化的模型，也不能太大而带来训练模型的时间花销，一般来说取5到10就能满足要求。

#### 集成方法选择

- 平均法

  在回归任务中，输出为一个数值，可以使用平均法来处理多个神经网络的输出值。下面公式中的$h_i(x)$表示第i个神经网络的输出，$H(x)$表示集成后的输出。

  - 简单平均法：所有值加起来除以N。
    $$H(x)=\frac{1}{N} \sum_{i=1}^N h_i(x)$$

  - 加权平均法：给每个输出值一个人为定义的权重。
  $$H(x)=\sum_{i=1}^N w_i \cdot h_i(x)$$

  权重值如何给出呢？假设第一个神经网络的准确率为80%，第二个为85%，我们可以令：

  $$w_1=0.8,w_2=0.85$$

  这样准确率高的网络会得到较大的权重值。

- 投票法

  对于分类任务，将会从类别标签集合$\\{c_1, c_2, ...,c_n\\}$中预测出一个值，多个神经网络可能会预测出不一样的值，此时可以采样投票法。

  - 绝对多数投票法（majority voting）

    当有半数以上的神经网路预测出同一个类别标签时，我们可以认为此预测有效。如果少于半数，则可以认为预测无效。

    比如9个神经网络，5个预测图片上的数字为7，则最终结果就是7。如果有4个神经网络预测为7，3个预测为4，2个预测为1，则认为预测失败。

  - 加权投票法(weighted voting)

    与加权平均法类似。

  - 相对多数投票法（plurality voting）

    即得票最多的标签获胜。如果有多个标签获得相同的票数，随机选一个。

- 学习法

  学习法，就是用另外一个神经网络，通过训练的方式，把9个神经网路的输出结果作为输入，把图片的真实数字作为标签，得到一个强学习器。

### 总结
深度网络的学习能力强的特点，会造成网络对样本数据过分拟合，从而造成泛化能力不足，因此我们需要一些手段来改善网络的泛化能力。

神经网络是基于感知机的扩展，可以理解为有很多隐藏层的神经网络。多层神经网络和深度神经网络其实也是指的一个东西，有时也叫做多层感知机（Multi-Layer perceptron,MLP）。

深度神经网络目前是许多人工智能应用的基础。由于深度神经网络在语音识别和图像识别上的突破性应用，使用深度神经网络的应用量有了爆炸性的增长。这些深度神经网络被部署到了从自动驾驶汽车、癌症检测到复杂游戏等各种应用中。在这许多领域中，深度神经网络能够超越人类的准确率。

深度神经网络的出众表现源于它能使用统计学习方法从原始感官数据中提取高层特征，在大量的数据中获得输入空间的有效表征。这与之前使用手动提取特征或专家设计规则的方法不同。

# miniFramework实验

## 关于miniFramework

百度miniFramework可看到官方网站。

![avatar](\picture\1.png)

点击可进入创作者的github上查看软件相关内容，甚至可以加入开发。

![avatar](\picture\2.png)

运行ch13中的代码可识别数字。

![avatar](\picture\3.png)

# 总结

人工智能概论教了很多高精尖的知识，在对各种神经网路的处理方面也进行了详细的讲解。通过一个个实例和图片，充分表现了人工智能概论的厉害之处，

通过提出问题到解决方案，再到原理分析，最后通过可视化理解。针对性的对每一个知识点进行说明，轻松的就能理解相关的知识点。

还通过配套的Python代码，以及一些必要的科学计算库和绘图库，使得大家可以从零开始搭建自己的知识体系，从简单到复杂，一步步理解深度学习中的众多知识点。

其次还有大量的示意图，可以通过这些示意图快速而深刻地理解知识点，能够从真正的“零”开始，对神经网络、深度学习有基本的了解，并能动手实践。

明白了可以判断哪些任务是机器学习可以实现的，哪些是科学幻想，深刻了解神经网络和深度学习的基本理论，一定程度上培养举一反三的解决实际问题的能力，通过资料查找得到自学更复杂模型和更高级内容的能力。

# 学习心得

想要很好的完成其中一些内容的学习，得学过高等数学中的线性代数与微分，就能充分的理解其中一些数学公式以及相似变换的含有，对内容可以更好的理解。

还要有一定的编程基础，可以不会Python语言，因为可以从示例代码中学得，但是要会用相关的编程软件，亲自将其中的代码再运行一遍，增加印象。

得拥有思考和动手的学习模式，在思考与动手中完成这一科的学习。