 # 人工智能概论报告
 >>>>>>> ### step 1简要回顾总结
  ## 举例说明卷积神经网络的应用
#### 卷积神经网络
##### 简介
+ 是一类包含卷积计算且具有深度结构的前馈神经网络，是深度学习（deep learning）的代表算法之一。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”。
+ 对卷积神经网络的研究：
    - 二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络。
    - 在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于计算机视觉、自然语言处理等领域。 
+ 卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征，例如像素和音频进行学习、有稳定的效果且对数据没有额外的特征工程（feature engineering）要求。
  
#### 结构
+ 输入层：
    - 一对卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样。
    - 二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组。
    - 三维卷积神经网络的输入层接收四维数组。
    - 由于卷积神经网络在计算机视觉领域应用较广，因此许多研究在介绍其结构时预先假设了三维输入数据，即平面上的二维像素点和RGB通道。
+ 隐含层：包含卷积层、池化层和全连接层3类常见构筑。卷积层中的卷积核包含权重系数，而池化层不包含权重系数，因此在文献中，池化层可能不被认为是独立的层
    - 卷积层：
        - 卷积核：对输入数据进行特征提取。
        - 卷积层参数：卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数 。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。填充依据其层数和目的可分为四类：有效填充、相同填充/半填充、全填充、任意填充。
        - 激励函数
    - 池化层：在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。池化层选取池化区域与卷积核扫描特征图步骤相同，由池化大小、步长和填充控制。
        - Lp池化：是一类受视觉皮层内阶层结构启发而建立的池化模型。
        - 随机/混合池化：是Lp池化概念的延伸。
        - 谱池化：是基于FFT的池化方法，可以和FFT卷积一起被用于构建基于FFT的卷积神经网络。 
    - 全连接层：卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层位于卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去空间拓扑结构，被展开为向量并通过激励函数。
+ 输出层：卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心坐标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。

#### 应用：
1. 计算机视觉：
    + 图像识别：卷积神经网络长期以来是图像识别领域的核心算法之一，并在学习数据充足时有稳定的表现。对于一般的大规模图像分类问题，卷积神经网络可用于构建阶层分类器，也可以在精细分类识别中用于提取图像的判别特征以供其它分类器进行学习。对于后者，特征提取可以人为地将图像的不同部分分别输入卷积神经网络，也可以由卷积神经网络通过非监督学习自行提取。
  + [![D6oUsA.png](https://s3.ax1x.com/2020/11/29/D6oUsA.png)](https://imgchr.com/i/D6oUsA)
    + 物体识别：卷积神经网络可以通过三类方法进行物体识别：滑动窗口、选择性搜索和YOLO。
    + 行为认为
    + 姿态估计
    + 神经风格迁移：是在给定的两份图像的基础上创作第三份图像，并使其内容和风格与给定的图像尽可能地接近。![](3.png)
2. 自然语言处理:
    + 在语音处理领域，卷积神经网络的表现被证实优于隐马尔可夫模型、高斯混合模型和其它一些深度算法。有研究使用卷积神经网络和HMM的混合模型进行语音处理，模型使用了小的卷积核并将替池化层用全连接层代替以提升其学习能力。卷积神经网络也可用于语音合成和语言建模.
3. 其他:
    + 物理学：在高能物理学中，卷积神经网络被用于粒子对撞机输出的喷流图的分析和特征学习，有关研究包括夸克/胶子分类、W玻色子识别和中微子相互作用研究等。卷积神经网络在天体物理学中也有应用，有研究使用卷积神经网络对天文望远镜图像进行星系形态学分析和提取星系模型参数。利用迁移学习技术，预训练的卷积神经网络可以对LIGO数据中的噪声进行检测，为数据的预处理提供帮助 。
    + 
    + 遥感科学：被认为在解析遥感图像的几何、纹理和空间分布特征时，有计算效率和分类准确度方面的优势。依据遥感图像的来源和目的，卷积神经网络被用于下垫面使用和类型改变研究 以及物理量，例如海冰覆盖率的遥感反演。
    + 
    + 大气科学：卷积神经网络被用于数值模式的统计降尺度和格点气候数据的极端天气检测。在统计降尺度方面，由SRCNN堆叠的多层级的降尺度系统可以将插值到高分辨率的原始气象数据和高分辨率的数字高程模型作为输入，并输出高分辨率的气象数据，其准确率超过了传统的空间分解误差订正方法。在极端天气检测方面，仿AlexNet结构的卷积神经网络在监督学习和半监督学习中被证实能以很高的准确度识别气候模式输出和再分析数据中的热带气旋、大气层河流和锋面现象。
    + 
    + 包含卷积神经网络的编程模块：现代主流的机器学习库和界面，包括TensorFlow、Keras、Thenao、Microsoft-CNTK等都可以运行卷积神经网络算法。此外一些商用数值计算软件，例如MATLAB也有卷积神经网络的构建工具可用。
 ## 举例说明BP神经网络的应用
#### 简介
+ 人工神经网络（Artificial Neural Netork，ANN）：由简单神经元经过相互连接形成网状结构，通过调节各连接的权重值改变连接的强度，进而实现感知判断。
+ 反向传播（Back Propagation，BP）算法：进一步推动了神经网络的发展。
  
##### 反向传播神经网络的应用：
1. BP神经网络：按照误差逆向传播算法训练的多层前馈神经网络。
2. 基本原理：
    + 工神经网络无需事先确定输入输出之间映射关系的数学方程，仅通过自身的训练，学习某种规则，在给定输入值时得到最接近期望输出值的结果。作为一种智能信息处理系统，人工神经网络实现其功能的核心是算法。
    + BP算法：一种按误差反向传播(简称误差反传)训练的多层前馈网络。基本思想是梯度下降法，利用梯度搜索技术，以期使网络的实际输出值和期望输出值的误差均方差为最小。
    + 基本BP算法包括信号的前向传播和误差的反向传播两个过程。即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。
3. 结构：BP网络是在输入层与输出层之间增加若干层(一层或多层)神经元，这些神经元称为隐单元，它们与外界没有直接的联系，但其状态的改变，则能影响输入与输出之间的关系，每一层可以有若干个节点。
4. 应用：
   + 基于神经网络（多层感知器）识别手写数字
    - 数据集为经典的MNIST。
    - 输入加载数据的代码 [![DysZrQ.png](https://s3.ax1x.com/2020/11/28/DysZrQ.png)](https://imgchr.com/i/DysZrQ)
   
    - 输入定义学习率、迭代次数、批大小、批数量（总样本数除以批大小）等参数，设置输入层大小为784，即将28*28的像素展开为一维行向量（一个输入图片784个值）。第一层和第二层神经元数量均为256，输出层的分类类别为0~9的数字，即10个类别的代码[![DysUaR.png](https://s3.ax1x.com/2020/11/28/DysUaR.png)](https://imgchr.com/i/DysUaR)
    
    - 使用tf.random_normal()生成模型权重值参数矩阵和偏置值参数，并将其分别存储于weights和biases变量中，并定义多层感知机的神经网络模型。[![Dysfit.png](https://s3.ax1x.com/2020/11/28/Dysfit.png)](https://imgchr.com/i/Dysfit)
  
    - 使用输入变量X初始化模型，定义损失函数为交叉熵，采用梯度下降法作为优化器（除此之外还可选MomentumOptimizer、AdagradOptimizer、AdamOptimizer）,并对模型中tf.placeholder定义的各种参数初始化。
    [![DysoQS.png](https://s3.ax1x.com/2020/11/28/DysoQS.png)](https://imgchr.com/i/DysoQS)

    - 将训练寄样本输入模型进行训练，并计算每个批次的平均损失，在每次迭代时输出模型的平均损失。[![Dyy9L4.png](https://s3.ax1x.com/2020/11/28/Dyy9L4.png)](https://imgchr.com/i/Dyy9L4)
    
    - 模型训练完成，使用测试集样本对其评估，并计算其精确率。[![Dyyh79.png](https://s3.ax1x.com/2020/11/28/Dyyh79.png)](https://imgchr.com/i/Dyyh79)
    
    - 模型的Accuracy结果为87.8%。 
   +  基于Elman神经网络的能源消耗预测 [![DyyztI.png](https://s3.ax1x.com/2020/11/28/DyyztI.png)](https://imgchr.com/i/DyyztI) 
    - 预测实验结果图[![Dy6Pc8.png](https://s3.ax1x.com/2020/11/28/Dy6Pc8.png)](https://imgchr.com/i/Dy6Pc8)
    - 100天预测结果走势比对图[![Dy6c4I.png](https://s3.ax1x.com/2020/11/28/Dy6c4I.png)](https://imgchr.com/i/Dy6c4I)
  
###  神经网络的基本工作原理
#### 神经元细胞的数学模型
 + 输入 input ：是外界输入信号，一般是一个训练数据样本的多个属性。
+ 权重 weights：每个输入信号的权重值。
+ 偏移 bias：b实际就是那个临界值。（用结果来解释是b是偏移值，使直线能够沿Y轴上下移动）
+ 求和计算 sum + 激活函数 activation：激活函数都是有一个渐变的过程，也就是说是个曲线。
+ 小结：    一个神经元：      
+ 可以有多个输入       
+ 只能有一个输出，这个输出可以同时输入给多个神经元      
+ w的数量和输入的数量一致     
+ 只有一个b   
+ w和b有人为的初始值，在训练过程中被不断修改 
+ 激活函数不是必须有的，亦即A可以等于Z   
+ 一层神经网络中的所有神经元的激活函数必须一致
 #### 神经网络的训练过程
1. 单层神经网络模型：这是一个单层的神经网络，有m个输入 (这里m=3)，有n个输出 (这里n=2)。在单个神经元里，b是个值。但是在神经网络中，我们把b的值永远设置为1，而用b到每个神经元的权值来表示实际的偏移值，亦即(b1,b2)，这样便于矩阵运算。   
+ $(x1,x2,x3)$是一个样本数据的三个特征值    
+ $(w11,w12,w13)$是$(x1,x2,x3)$到$n1$的权重   
+ $(w21,w22,w23)$是$(x1,x2,x3)$到$n2$的权重    
+ $b1$是$n1$的偏移      
+ $b2$是$n2$的偏移2. 前提条件：   
+ 有训练数据   
+ 根据数据的规模、领域，建立了神经网络的基本结构。    
+ 定义好损失函数来合理地计算误差3. 神经网络中的矩阵运算#### 神经网络的主要功能1. 回归/拟合
   分类
#### 激活函数
1. 生理学上的例子   
  [![DyfyVS.png](https://s3.ax1x.com/2020/11/28/DyfyVS.png)](https://imgchr.com/i/DyfyVS)
2. 激活函数的作用:一个没有激活函数的神经网络将只不过是一个线性回归模型.同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据。    
+ Sigmoid激活函数：$$a = \frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(wx+b)}}$$ 
3. 激活函数的另一个重要特征是：它应该是可导的。   
#### 深度神经网络与深度学习：三层以上的网络称为深度神经网络。
1. 卷积神经网络(CNN)对于图像类的机器学习问题，最有效的就是卷积神经网络2. 循环神经网络(RNN)对于语言类的机器学习问题，最有效的就是循环神经网络。
#### Deep Learning的训练过程
1. 使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）2. 自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）
2.    
### 总结
+ 这个章节介绍的是有监督学习的神经网络机器学习过程。神经网络模型：![](05.png)，神经网络模型是逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。
+ 反向传播算法是比较快速可以计算出结果的。在计算神经网络预测结果的时候采用一种正向传播方法（从第一层开始正向一层一层进行计算，直到最后一层的 hθ(x)。）。而反向传播算法（首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。）
+ 反向传播算法推导：    
+  求deltaTheta   
+  求δ(误差) 链式求导
+ 即首先正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。
### 神经网络中的三个基本概念
这三大概念是：反向传播，梯度下降，损失函数。

神经网络训练的最基本的思想就是：先“猜”一个结果，称为预测结果 $a$，看看这个预测结果和事先标记好的训练集中的真实结果 $y$ 之间的差距，然后调整策略，再试一次，这一次就不是“猜”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即 $|a-y|\rightarrow 0$，就结束训练。

在神经网络训练中，我们把“猜”叫做初始化，可以随机，也可以根据以前的经验给定初始值。即使是“猜”，也是有技术含量的。
简单总结一下反向传播与梯度下降的基本工作原理：

1.初始化；
2.正向计算；
3.损失函数为我们提供了计算损失的方法；
4.梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5.反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6.Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。
### 梯度下降
#### 梯度下降的数学理解
梯度下降的数学公式：

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

其中：

* $\theta_{n+1}$：下一个值；
* $\theta_n$：当前值；
* $-$：减号，梯度的反向；
* $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长；
* $\nabla$：梯度，函数当前位置的最快上升点；
* $J(\theta)$：函数。
#### 梯度下降的三要素
1.当前点；
2.方向；
3.步长。
+ 梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。
##### 单变量函数的梯度下降
[![D6f0O0.png](https://s3.ax1x.com/2020/11/29/D6f0O0.png)](https://imgchr.com/i/D6f0O0)
##### 双变量的梯度下降
[![D6fsTU.png](https://s3.ax1x.com/2020/11/29/D6fsTU.png)](https://imgchr.com/i/D6fsTU)
 [![D6f6kF.png](https://s3.ax1x.com/2020/11/29/D6f6kF.png)](https://imgchr.com/i/D6f6kF)

##### 学习率η的选择
在公式表达时，学习率被表示为$\eta$。在代码里，我们把学习率定义为learning_rate，或者eta。针对上面的例子，试验不同的学习率对迭代情况的影响
* [![D6fRp9.png](https://s3.ax1x.com/2020/11/29/D6fRp9.png)](https://imgchr.com/i/D6fRp9)
学习率太大，迭代的情况很糟糕，在一条水平线上跳来跳去，永远也不能下降。
*[![D6fhOx.png](https://s3.ax1x.com/2020/11/29/D6fhOx.png)](https://imgchr.com/i/D6fhOx)
学习率大，会有这种左右跳跃的情况发生，这不利于神经网络的训练。
* [![D6f7kD.png](https://s3.ax1x.com/2020/11/29/D6f7kD.png)](https://imgchr.com/i/D6f7kD)
学习率合适，损失值会从单侧下降，4步以后基本接近了理想值。
* [![D6fX6I.png](https://s3.ax1x.com/2020/11/29/D6fX6I.png)](https://imgchr.com/i/D6fX6I)
学习率较小，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。
### 损失函数
##### 概念
在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本书中，使用“损失函数”和“Loss Function”这两个词汇，具体的损失函数符号用 $J$ 来表示，误差值用 $loss$ 表示。

“损失”就是所有样本的“误差”的总和，亦即（$m$ 为样本数）：

$$损失 = \sum^m_{i=1}误差_i$$

$$J = \sum_{i=1}^m loss_i$$
##### 损失函数的作用
损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
使用损失函数具体步骤：
1.用随机值初始化前向计算公式的参数；
2.代入样本，计算输出的预测值；
3.用损失函数计算预测值和标签值（真实值）的误差；
4.根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5.进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。
##### 机器学习常用损失函数
符号规则：$a$ 是预测值，$y$ 是样本标签值，$loss$ 是损失函数值。

Gold Standard Loss，又称0-1误差 $$ loss=\begin{cases} 0 & a=y \\ 1 & a \ne y \end{cases} $$

绝对值损失函数

$$ loss = |y-a| $$

Hinge Loss，铰链/折页损失函数或最大边界损失函数，主要用于SVM（支持向量机）中
$$ loss=\max(0,1-y \cdot a) \qquad y=\pm 1 $$

Log Loss，对数损失函数，又叫交叉熵损失函数(cross entropy error)
$$ loss = -[y \cdot \ln (a) + (1-y) \cdot \ln (1-a)] \qquad y \in \{ 0,1 \} $$

Squared Loss，均方差损失函数 $$ loss=(a-y)^2 $$

Exponential Loss，指数损失函数 $$ loss = e^{-(y \cdot a)} $$
##### 损失函数图像理解
1. 用二维函数图像理解单变量对损失函数的影响
2. 用等高线图理解双变量对损失函数影响#### 神经网络中常用损失函数 均方差函数，主要用于回归：该函数就是最直观的一个损失函数了，计算预测值和真实值之间的欧式距离。预测值和真实值越接近，两者的均方差就越小。
   + 公式
  + $$ loss = {1 \over 2}(z-y)^2 \tag{单样本} $$   
   + $$ J=\frac{1}{2m} \sum_{i=1}^m (z_i-y_i)^2 \tag{多样本}$$
  + 工作原理：   
   + 要想得到预测值a与真实值y的差距，最朴素的想法就是用$Error=a_i-y_i$。    
  + 对于单个样本来说，这样做没问题，但是多个样本累计时，$a_i-y_i$有可能有正有负，误差求和时就会导致相互抵消，从而失去价值。所以有了绝对值差的想法，即$Error=|a_i-y_i|$。
    交叉熵函数，主要用于分类：是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。
    + 在信息论中，交叉熵是表示两个概率分布p,q的差异，其中p表示真实分布，q表示非真实分布，那么H(p,q)就称为交叉熵
  + 公式:$$H(p,q)=\sum_i p_i \cdot log {1 \over q_i} = - \sum_i p_i \log q_i$$
  + 交叉熵的由来：    
  + 信息量：$$I(x_j) = -\log (p(x_j))$$（$x_j$：表示一个事件；$p(x_j)$：表示$x_i$发生的概率；$I(x_j)$：信息量，$x_j$越不可能发生时，它一旦发生后的信息量就越大）   
   + 熵：$$H(p) = - \sum_j^n p(x_j) \log (p(x_j))$$   
   + 相对熵（KL散度）：,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用KL散度来衡量这两个分布的差异，这个相当于信息论范畴的均方差。      
    + 公式：$$D_{KL}(p||q)=\sum_{j=1}^n p(x_j) \log{p(x_j) \over q(x_j)}$$（n为时间的所有可能性。D的值越小，表示q分布和p分布越接近）
  + 交叉熵：$$D_{KL}(p||q)=\sum_{j=1}^n p(x_j) \log{p(x_j)} - \sum_{j=1}^n p(x_j) \log q(x_j)$$ $$ =- H(p(x)) + H(p,q) $$
  + 二分类问题交叉熵
  + 多分类问题交叉熵
  + 不能使用均方差作为分类问题的损失函数原因   
 + 回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。    
  + 分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果很复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。
+  运算代码的结果：
  ![](14.png)

  二者都是非负函数，极值在底部
  #### 均方差函数
  ##### 工作原理
要想得到预测值 $a$ 与真实值 $y$ 的差距，最朴素的想法就是用 $Error=a_i-y_i$。

对于单个样本来说，这样做没问题，但是多个样本累计时，$a_i-y_i$ 可能有正有负，误差求和时就会导致相互抵消，从而失去价值。所以有了绝对值差的想法，即 $Error=|a_i-y_i|$ 。这看上去很简单，并且也很理想两种损失函数的比较如表所示。
绝对值损失函数与均方差损失函数的比较
[![D6Ik1H.png](https://s3.ax1x.com/2020/11/29/D6Ik1H.png)](https://imgchr.com/i/D6Ik1H)



### step1小结
本章主要学习了
1. 神经元

2. 激活功能

3. 激活功能的类型

4. 神经网络如何工作

5. 神经网络如何学习（反向传播）

6. 梯度下降

7. 随机梯度下降

8. 随机梯度下降训练神经网络
通过本章的学习主要了解到神经网络是一个全新的新概念，它还有很大潜力。可以应用于各种不同的概念中，并在测试阶段通过特定的反向传播和纠错机制进行学习。这些多层系统也许可以有一天仅靠学习来减少发生错误的可能性，而无需人工纠正。
### step1心得体会
通过对于学习回顾，我发现了自己的很多不足，同时回顾的过程也是重新梳理知识的过程，有了很多新的收获。


>>>>>>> ### step 2简要回顾总结
##### 这一部分主要讲了线性回顾在人工智能方面的应用，所以主要围绕线性回归问题展开讲述
#### 单入单出的单层神经网络 - 单变量线性回归
* 线性回归中的一些术语
标签：
是我们要预测的真实事物，在上面例子线性回归中就是应变量y
特征：
是用于描述数据的输入变量，在上面例子线性回归中就是自变量x的取值集合{x1、x2、x3、…xn}
样本：
是数据的特定实例：x的一个取值
有标签样本就是{特征，标签}：{x,y}，用来训练模型
无………………{特征，？}：{x,?},用来对新数据进行预测
模型：
可以将样本映射到预测标签:y’是由模型的内部参数定义，内部参数则是通过不断地学习而得到。
训练：
训练模型是指通过有标签的样本来学习所有的权重（x的系数）和偏差（截距）的理想值

在监督式学习中，机器学习算法可以通过检查多个样本并尝试找出可最大限度地减少损失的模型。（也被称之为经验风险最小化）
损失：
是对糟糕预测的惩罚，是一个对于单个样本而言模型预测的准确程度的数值指标。

训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。
[![D6o0dP.png](https://s3.ax1x.com/2020/11/29/D6o0dP.png)](https://imgchr.com/i/D6o0dP)

损失函数：
L1损失：基于模型预测值与标签的实际值之差的绝对值为指标
L2损失：基于模型预测值与标签的实际值之差的平均平方损失为指标（均方误差MSE）
[![D6oBIf.png](https://s3.ax1x.com/2020/11/29/D6oBIf.png)](https://imgchr.com/i/D6oBIf)

* 训练模型的迭代法
  示意图如下：
  [![D6osJS.png](https://s3.ax1x.com/2020/11/29/D6osJS.png)](https://imgchr.com/i/D6osJS)
  由上图可知，模型训练的要点：(对于单变量线性回归问题y=wx+b)
1.对权重w和偏差b进行初始猜测；
2.反复调整这些猜测；
3.一直迭代到获得损失可能最低的权重和偏差为止。
当不断迭代，直到损失不再发生变化或十分缓慢，就说这个模型已收敛。
上述线性回归问题产生的损失与权重图为凸型，最低点斜率为零处就是损失函数收敛之处。示意图如下：
[![D6oyRg.png](https://s3.ax1x.com/2020/11/29/D6oyRg.png)](https://imgchr.com/i/D6oyRg)
这里采用的是梯度下降法。
梯度：就是一个矢量，表示某一函数在该点处的方向导数沿着该方向取得最大值（即函数在该点处沿着该方向（梯度方向）变化最快，变化率最大）
（此处不再赘述梯度下降法的具体原理高数有学）
###### 需要注意一下帝都为矢量，有方向和大小。
用梯度乘以一个名叫学习速率（也称步长）的标量，用来确定下一个权重的取值。
比如梯度大小为2.5，学习速率为0.05，则梯度下降算法会选择距离前一个点0.125的位置作为下一个点。
选取需要适中。
[![D6oIiT.png](https://s3.ax1x.com/2020/11/29/D6oIiT.png)](https://imgchr.com/i/D6oIiT)
* 单变量线性回归实践结果如下
  准备数据
  [![D6oqy9.png](https://s3.ax1x.com/2020/11/29/D6oqy9.png)](https://imgchr.com/i/D6oqy9)
  构建模型
  [![D6oLLR.png](https://s3.ax1x.com/2020/11/29/D6oLLR.png)](https://imgchr.com/i/D6oLLR)
  进行预测
  [![D6oXe1.png](https://s3.ax1x.com/2020/11/29/D6oXe1.png)](https://imgchr.com/i/D6oXe1)
#### 最小二乘法
最小二乘法，也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最小二乘法来表达。

1801年，意大利天文学家朱赛普·皮亚齐发现了第一颗小行星谷神星。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥尔伯斯根据高斯计算出来的轨道重新发现了谷神星。

高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中。法国科学家勒让德于1806年独立发明“最小二乘法”，但因不为世人所知而默默无闻。勒让德曾与高斯为谁最早创立最小二乘法原理发生争执。

1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，因此被称为高斯-马尔可夫定理。

* 数学原理
线性回归试图学得：
$$z_i=w \cdot x_i+b \tag{1}$$

使得：
$$z_i \simeq y_i \tag{2}$$

其中，$x_i$ 是样本特征值，$y_i$ 是样本标签值，$z_i$ 是模型预测值。
均方差(MSE - mean squared error)是回归任务中常用的手段：
$$
J = \frac{1}{2m}\sum_{i=1}^m(z_i-y_i)^2 = \frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2 \tag{3}
$$

$J$ 称为损失函数。实际上就是试图找到一条直线，使所有样本到直线上的残差的平方和最小。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/mse.png" />
#### 计算z的梯度

根据公式2：
$$
\frac{\partial loss}{\partial z_i}=z_i - y_i \tag{3}
$$

* 计算 $w$ 的梯度

我们用 $loss$ 的值作为误差衡量标准，通过求 $w$ 对它的影响，也就是 $loss$ 对 $w$ 的偏导数，来得到 $w$ 的梯度。由于 $loss$ 是通过公式2->公式1间接地联系到 $w$ 的，所以我们使用链式求导法则，通过单个样本来求导。

根据公式1和公式3：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i \tag{4}
$$

* 计算 $b$ 的梯度

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i \tag{5}
$$

#### 多样本计算
* 1. 前后两个相邻的样本很有可能会对反向传播产生相反的作用而互相抵消。假设样本1造成了误差为 $0.5$，$w$ 的梯度计算结果是 $0.1$；紧接着样本2造成的误差为 $-0.5$，$w$ 的梯度计算结果是 $-0.1$，那么前后两次更新 $w$ 就会产生互相抵消的作用。
2. 在样本数据量大时，逐个计算会花费很长的时间。由于我们在本例中样本量不大（200个样本），所以计算速度很快，觉察不到这一点。在实际的工程实践中，动辄10万甚至100万的数据量，轮询一次要花费很长的时间。

如果使用多样本计算，就要涉及到矩阵运算了，而所有的深度学习框架，都对矩阵运算做了优化，会大幅提升运算速度。打个比方：如果200个样本，循环计算一次需要2秒的话，那么把200个样本打包成矩阵，做一次计算也许只需要0.1秒。
* 前相计算
  由于有多个样本同时计算，所以我们使用 $x_i$ 表示第 $i$ 个样本，$X$ 是样本组成的矩阵，$Z$ 是计算结果矩阵，$w$ 和 $b$ 都是标量：

$$
Z = X \cdot w + b \tag{1}
$$

把它展开成3个样本（3行，每行代表一个样本）的形式：

$$
X=\begin{pmatrix}
    x_1 \\\\ 
    x_2 \\\\ 
    x_3
\end{pmatrix}
$$

$$
Z= 
\begin{pmatrix}
    x_1 \\\\ 
    x_2 \\\\ 
    x_3
\end{pmatrix} \cdot w + b 
=\begin{pmatrix}
    x_1 \cdot w + b \\\\ 
    x_2 \cdot w + b \\\\ 
    x_3 \cdot w + b
\end{pmatrix}
=\begin{pmatrix}
    z_1 \\\\ 
    z_2 \\\\ 
    z_3
\end{pmatrix} \tag{2}
$$

$z_1,z_2,z_3$ 是三个样本的计算结果。根据公式1和公式2，我们的前向计算`Python`代码可以写成：

```Python
    def __forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.w) + self.b
        return Z
```
`Python`中的矩阵乘法命名有些问题，`np.dot()`并不是矩阵点乘，而是矩阵叉乘，请读者习惯。

### 损失函数

用传统的均方差函数，其中，$z$ 是每一次迭代的预测输出，$y$ 是样本标签数据。我们使用 $m$ 个样本参与计算，因此损失函数为：

$$J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(z_i - y_i)^2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

我们假设每次有3个样本参与计算，即 $m=3$，则损失函数实例化后的情形是：

$$
\begin{aligned}
J(w,b) &= \frac{1}{2\times3}[(z_1-y_1)^2+(z_2-y_2)^2+(z_3-y_3)^2] \\\\
&=\frac{1}{2\times3}\sum_{i=1}^3[(z_i-y_i)^2]
\end{aligned} 
\tag{3}
$$

公式3中大写的 $Z$ 和 $Y$ 都是矩阵形式，用代码实现：

```Python
    def __checkLoss(self, dataReader):
        X,Y = dataReader.GetWholeTrainSamples()
        m = X.shape[0]
        Z = self.__forwardBatch(X)
        LOSS = (Z - Y)**2
        loss = LOSS.sum()/m/2
        return loss
```
`Python`中的矩阵减法运算，不需要对矩阵中的每个对应的元素单独做减法，而是整个矩阵相减即可。做求和运算时，也不需要自己写代码做遍历每个元素，而是简单地调用求和函数即可。
#### 梯度下降的三种形式
* 总共有BGD,SGD, MBGD三种梯度下降法
##### 批量梯度下降法BGD
  是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，其数学形式如下：
[![D6TPQH.png](https://s3.ax1x.com/2020/11/29/D6TPQH.png)](https://imgchr.com/i/D6TPQH)
* 优点：
全局最优解；易于并行实现；
* 缺点：
当样本数目很多时，训练过程会很慢。
[![D6Tiyd.png](https://s3.ax1x.com/2020/11/29/D6Tiyd.png)](https://imgchr.com/i/D6Tiyd)（迭代次数较少）
##### 随机梯度下降法SGD
* 由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。
将上面的能量函数写为如下形式：
[![D6TFOA.png](https://s3.ax1x.com/2020/11/29/D6TFOA.png)](https://imgchr.com/i/D6TFOA)
* 优点：
训练速度快；
* 缺点：
准确度下降，并不是全局最优；不易于并行实现。
[![D6TEwt.png](https://s3.ax1x.com/2020/11/29/D6TEwt.png)](https://imgchr.com/i/D6TEwt)（迭代次数较多）
##### 小批量梯度下降法MBGD
有上述的两种梯度下降法可以看出，其各自均有优缺点，在两种方法的性能之间取得一个折衷，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。
MBGD在每次更新参数时使用b个样本（b一般为10），其具体的伪代码形式为：
[![D6TeFf.png](https://s3.ax1x.com/2020/11/29/D6TeFf.png)](https://imgchr.com/i/D6TeFf)
#### 多入单出层多变量线性回归
建立多元线性回归模型时，为了保证回归模型具有优良的解释能力和预测效果，应首先注意自变量的选择，其准则是：

1. 自变量对因变量必须有显著的影响，并呈密切的线性相关；
2. 自变量与因变量之间的线性相关必须是真实的，而不是形式上的；
3. 自变量之间应具有一定的互斥性，即自变量之间的相关程度不应高于自变量与因变量之因的相关程度；
4. 自变量应具有完整的统计数据，其预测值容易确定。

#### 正规方程法
正规方程是求解线性回归的损失函数的最小二乘的一种分析方法。我们可以不使用Gradient Descent直接找出θ的值。当使用具有小特征的数据集时即n较小时，遵循此方法是一种有效且省时的选项。

#### 神经网络法
* 线性回归一样，神经网络实际上就是要训练找到合适的w 和 b。与线性回归一样，使用梯度下降(Grident Dscent)法，即可得到最优 的w和b。
前向传播和反向传播（Forward and Backward propagation）
* 前向传播其实很简单，就是如何堆砌这个神经网络，多少个Feature 输入，多少层神经网络，每层多少个神经元，每一层用什么激活函数。
最困难的是反向传播，类似于线性回归一样，我们的目的是要用合适的参数（W和b）使这个网络，或者说整个模型预测的值最接近真实的数值，换句话说就是预测值与真实值的差距最小。这个求这个差值的函数我们叫代价函数(Cost Function), 而反向传播就是通过预测结果，向前倒推每一层W和b的导数。通过这个导数我们就可以用梯度下降的方法训练出代价函数最小的W和b值。
反向传播涉及到了微分，和偏微分（偏导）递归等数学原理，虽然也不难，但是也并不在本文的讨论范围之内。不过好消息是在现在流行的深度学习工具中，比如在Tensorflow中, 我们只需要关心如何搭建这个网络（前向传播），工具会自动通过反向传播计算最优解
#### 样本特征数据标准化
* 神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，所以它对样本数据的要求比较苛刻。具体说明如下：
1. 样本的各个特征的取值要符合概率分布，即 $[0,1]$。
2. 样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小。
3. 神经网络假设所有的输入输出数据都是标准差为1，均值为0，包括权重值的初始化，激活函数的选择，以及优化算法的设计。
4. 数值问题
    标准化可以避免一些不必要的数值问题。因为激活函数sigmoid/tanh的非线性区间大约在 $[-1.7，1.7]$。意味着要使神经元有效，线性计算输出的值的数量级应该在1（1.7所在的数量级）左右。这时如果输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。 
5. 梯度更新 
    若果输出层的数量级很大，会引起损失函数的数量级很大，这样做反向传播时的梯度也就很大，这时会给梯度的更新带来数值问题。
6. 学习率
    如果梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据标准化，这样学习率就不必再根据数据范围作调整。对 $w_1$ 适合的学习率，可能相对于 $w_2$ 来说会太小，若果使用适合 $w_1$ 的学习率，会导致在 $w_2$ 方向上步进非常慢，从而消耗非常多的时间；而使用适合 $w_2$ 的学习率，对 $w_1$ 来说又太大，搜索不到适合 $w_1$ 的解。
* 标准化的常用方法

- Min-Max标准化（离差标准化），将数据映射到 $[0,1]$ 区间

$$x_{new}=\frac{x-x_{min}}{x_{max} - x_{min}} \tag{1}$$

- 平均值标准化，将数据映射到[-1,1]区间
   
$$x_{new} = \frac{x - \bar{x}}{x_{max} - x_{min}} \tag{2}$$

- 对数转换
$$x_{new}=\ln(x_i) \tag{3}$$

- 反正切转换
$$x_{new}=\frac{2}{\pi}\arctan(x_i) \tag{4}$$

- Z-Score法

把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。Z-Score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：

$$x_{new} = \frac{x_i - \bar{x}}{std} \tag{5}$$

- 中心化，平均值为0，无标准差要求
  
$$x_{new} = x_i - \bar{x} \tag{6}$$

- 比例法，要求数据全是正值

$$
x_{new} = \frac{x_k}{\sum_{i=1}^m{x_i}} \tag{7}
$$
#### step2 总结
关于单变量线性回归机器学习求解方面
上面的基础知识了解之后就可以解决这个单变量线性回归问题了。
使用Tensorflow进行算法设计与训练的核心步骤：
（1）准备数据
（2）构建模型
（3）训练模型
（4）进行预测
对于最小二乘法小二乘法就是找到一个或一组估计值，使得实际值与估计值的距离最小。假设平面内会有一系列点，然后我们求取一条线，使得这条线尽可能拟合这些点分布，这就是线性回归。这条线有多种找法，最小二乘法就是其中一种。最小二乘法其原理是找到一条线使得平面内的所有点到这条线的欧式距离和最小。线性指的是用一条线对数据进行拟合，距离代表的是数据误差，最小二乘法可以看做是误差最小化。
神经网络分析法是从神经心理学和认知科学研究成果出发，应用数学方法发展起来的一种具有高度并行计算能力、自学能力和容错能力的处理方法。
三种梯度下降法各有优劣，根据抢矿具体选择。

#### step2 心得
单变量线性回归问题是入门机器学习，了解机器学习步骤的好方法。所以一定要好好掌握。样本数量也对仿真神经网络精度的训练有着密切关系。
总体感觉线性回归这一章节内容很多但是至关重要还是继续加油！业精于勤而荒于嬉，行成于思而毁于随！

>>>>>>> ## step3 简要回顾总结
#### 线性二分类
二分类的代数原理

代数方式：通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5。

#### 基本公式回顾

下面我们以单样本双特征值为例来说明神经网络的二分类过程，这是用代数方式来解释其工作原理。

1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$
二分类的几何原理

几何方式：让所有正例样本处于直线的一侧，所有负例样本处于直线的另一侧，直线尽可能处于两类样本的中间。

#### 二分类函数的几何作用

* 二分类函数的最终结果是把正例都映射到图6-6中的上半部分的曲线上，而把负类都映射到下半部分的曲线上。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/sigmoid_binary.png"/>

图6-6 $Logistic$ 函数把输入的点映射到 $(0,1)$ 区间内实现分类

我们用正例来举例：

$$a = Logistic(z) = \frac{1}{1 + e^{-z}} > 0.5$$

做公式变形，两边取自然对数，可以得到：

$$z > 0$$

即：
$$
z = x_1 \cdot w_1 + x_2 \cdot w_2 + b > 0
$$

对上式做一下变形，把$x_2$放在左侧，其他项放在右侧（假设$w_2>0$，则不等号方向不变）：
$$
x_2 > - \frac{w_1}{w_2}x_1 - \frac{b}{w_2} \tag{5}
$$

简化一下两个系数，令$w'=-w1/w2,b'=-b/w2$：

$$
x_2 > w' \cdot x_1 + b' \tag{6}
$$

* 公式6用几何方式解释，就是：有一条直线，方程为 $z = w' \cdot x_1+b'$，所有的正例样本都处于这条直线的上方；同理可得所有的负例样本都处于这条直线的下方。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_binary_analysis.png" ch="500" />

#### 线性多分类
多分类过程

我们在此以具有两个特征值的三分类举例。可以扩展到更多的分类或任意特征值，比如在ImageNet的图像分类任务中，最后一层全连接层输出给分类器的特征值有成千上万个，分类有1000个。

1. 线性计算

$$z_1 = x_1 w_{11} + x_2 w_{21} + b_1 \tag{1}$$
$$z_2 = x_1 w_{12} + x_2 w_{22} + b_2 \tag{2}$$
$$z_3 = x_1 w_{13} + x_2 w_{23} + b_3 \tag{3}$$

2. 分类计算

$$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{4}
$$
$$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{5}
$$
$$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{6}
$$

3. 损失函数计算

单样本时，$n$表示类别数，$j$表示类别序号：

$$
\begin{aligned}
loss(w,b)&=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3) \\\\
&=-\sum_{j=1}^{n} y_j \ln a_j 
\end{aligned}
\tag{7}
$$

批量样本时，$m$ 表示样本数，$i$ 表示样本序号：

$$
\begin{aligned}
J(w,b) &=- \sum_{i=1}^m (y_{i1} \ln a_{i1} + y_{i2} \ln a_{i2} + y_{i3} \ln a_{i3}) \\\\
&=- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij}
\end{aligned}
 \tag{8}
$$
#### step3 小结
这是构建深度学习中的神经网络的基础算法。
#### step3心得体会
这就是我理解的机器学习中的分类算法中的二分类算法--逻辑回归的原理。
目前工业界最能产生价值的是机器学习中的监督学习， 场景有推荐系统，反欺系统诈等等。其中二分类算法应用的尤其之多。

>>>>> ##   step4简要回顾总结
#### 激活函数
* 常用激活函数：
（1）Sigmoid函数
sigmoid函数可以将输入的整个实数范围内的任意值映射到[0,1]范围内，当输入值较大时，会返回一个接近于1的值，当输入值较小时，则返回一个接近于0的值。在tensorflow中，用tf.sigmoid(x)直接调用这个函数使用。
Sigmoid函数的数学公式和函数图像如下：
[![D6TMlQ.png](https://s3.ax1x.com/2020/11/29/D6TMlQ.png)](https://imgchr.com/i/D6TMlQ)
[![D6TQyj.png](https://s3.ax1x.com/2020/11/29/D6TQyj.png)](https://imgchr.com/i/D6TQyj)
* 优点：输出在映射区间(0,1)内单调连续，非常适合用作输出层，并且比较容易求导。
* 缺点：其解析式中含有幂运算，计算机求解时相对比较耗时，对于规模比较大的深度网络，会较大地增加训练时间。
  
（2）Softmax函数
softmax函数实际上是在sigmoid函数上做一个推广，它可以将所有输出映射成概率的形式，即值在[0,1]范围且概率总和为1。tensorflow中可以用tf.nn.softmax()来调用。

Softmax函数的数学公式如下：
[![D6TGT0.png](https://s3.ax1x.com/2020/11/29/D6TGT0.png)](https://imgchr.com/i/D6TGT0)
假如输入变量为[1.5,4.4,2.0]，经过softmax函数激活后，输出为[0.04802413, 0.87279755, 0.0791784 ]，分别对应属于1、2、3类的概率。代码实现则为：
tf.nn.softmax(tf.constant([[1.5,4.4,2.0]]))。

（3）Tanh函数
tanh函数相似于sigmoid函数，但它能将值映射到[-1,1]的范围。于sigmoid函数相比，它的输出均值是0，使得其收敛速度要比sigmoid快，减少迭代次数，但幂运算的问题依然存在。
Tanh函数数学公式和函数图像如下：
[![D6TYkV.png](https://s3.ax1x.com/2020/11/29/D6TYkV.png)](https://imgchr.com/i/D6TYkV)
[![D6TtYT.png](https://s3.ax1x.com/2020/11/29/D6TtYT.png)](https://imgchr.com/i/D6TtYT)

（4）relu函数
relu函数，是目前被使用最为频繁得激活函数，relu函数在x < 0时，输出始终为0。由于x > 0时，relu函数的导数为1，即保持输出为x，所以relu函数能够在x > 0时保持梯度不断衰减，从而缓解梯度消失的问题，还能加快收敛速度。

relu函数数学公式和函数图像如下：
[![D6TNfU.png](https://s3.ax1x.com/2020/11/29/D6TNfU.png)](https://imgchr.com/i/D6TNfU)
[![D6TapF.png](https://s3.ax1x.com/2020/11/29/D6TapF.png)](https://imgchr.com/i/D6TapF)

#### 单入单出的双层神经网络 - 非线性回归
* 为什么要用两层神经网络？
首先，一层神经网络肯定不能完成这个复杂函数的拟合过程。因为一层神经网络，只能完成线性任务。这里的“线性任务”的定义，从简单到复杂，列表如下：
[![Dcw76g.png](https://s3.ax1x.com/2020/11/29/Dcw76g.png)](https://imgchr.com/i/Dcw76g)
* 所谓的“高阶”，指的是特征变量其实只有一个x1，但是把x1的平方也算作第二个特征向量。比如一栋房子的长度x1，宽度x2，占地面积x3=x1*x2。这里的x3并不是独立存在的，真正的自变量只有x1和x2。
这些高次线性回归问题，可以用单层的神经网络来解决，但是是有前提条件的，即假设函数必须和实际问题吻合。满足这个条件的实际工程问题并不多见，并且这种情况完全可以用两层的神经网络来解决，所以我们没有在单层的神经网络中涉及这个问题。

* 对于多层神经网络也是如此，我们要完成拟合任务，而不是分类，所以用不到激活/分类函数。通常把激活函数和分类函数混淆在一起说，如果明确地区分二者，则可以这样说：神经网络的最后一层不用激活函数，只可能用到分类函数。Sigmoid既是激活函数，又是分类函数，是个特例。
* 对激活函数在多层神经网络中做线性分类或回归的进一步解释：
在两层神经网络的输出层，可以用和单层神经网络一样的结构来完成分类任务，而用隐层来完成非线性到线性的转换工作。我们可以通过以下几张图的比较来理解一下非线性到线性的转换。
假设有两组点组成的红蓝两色曲线如下图：
[![DcrRv8.png](https://s3.ax1x.com/2020/11/29/DcrRv8.png)](https://imgchr.com/i/DcrRv8)
* 一层神经网络
[![DcrjrF.png](https://s3.ax1x.com/2020/11/29/DcrjrF.png)](https://imgchr.com/i/DcrjrF)
* 两层神经网络
  [![Dcsain.png](https://s3.ax1x.com/2020/11/29/Dcsain.png)](https://imgchr.com/i/Dcsain)
* 第二层也是一个线性的变化，关键在于第一层添加了激活函数之后，做到了坐标转换和空间扭曲：（第一层计算，把坐标空间扭曲，然后第二层神经网络轻松地画了一条直线，就把二者完美分开了）
  [![DcsRiR.png](https://s3.ax1x.com/2020/11/29/DcsRiR.png)](https://imgchr.com/i/DcsRiR)
* 关于使用均方差和交叉熵损失你函数的一个总结
[![DcyCwQ.png](https://s3.ax1x.com/2020/11/29/DcyCwQ.png)](https://imgchr.com/i/DcyCwQ)
交叉熵函数是用于分类的，均方差函数是用于拟合的，可以理解为计算拟合的点和样本标签点的距离之平方和。
拟合/回归的目的是减少预测值和样本标签值之间的差距，差距通过均方差的欧氏距离来表示。

* 反向传播
  [![Dcy0kd.png](https://s3.ax1x.com/2020/11/29/Dcy0kd.png)](https://imgchr.com/i/Dcy0kd)
   回归模型的评估标准

回归问题主要是求值，评价标准主要是看求得值与实际结果的偏差有多大，所以，回归问题主要以下方法来评价模型。

* 平均绝对误差

MAE（Mean Abolute Error）。

$$MAE=\frac{1}{m} \sum_{i=1}^m \lvert a_i-y_i \rvert \tag{1}$$

对异常值不如均方差敏感，类似中位数。

* 绝对平均值率误差

MAPE（Mean Absolute Percentage Error）。

$$MAPE=\frac{100}{m} \sum^m_{i=1} \left\lvert {a_i - y_i \over y_i} \right\rvert \tag{2}$$

* 和方差

SSE（Sum Squared Error）。

$$SSE=\sum_{i=1}^m (a_i-y_i)^2 \tag{3}$$

得出的值与样本数量有关系，假设有1000个测试样本，得到的值是120；如果只有100个测试样本，得到的值可能是11，我们不能说11就比120要好。

* 均方差

MSE（Mean Squared Error）。

$$MSE = \frac{1}{m} \sum_{i=1}^m (a_i-y_i)^2 \tag{4}$$

就是实际值减去预测值的平方再求期望，没错，就是线性回归的代价函数。由于MSE计算的是误差的平方，所以它对异常值是非常敏感的，因为一旦出现异常值，MSE指标会变得非常大。MSE越小，证明误差越小。

* 均方根误差

RMSE（Root Mean Squard Error）。

$$RMSE = \sqrt{\frac{1}{m} \sum_{i=1}^m (a_i-y_i)^2} \tag{5}$$
#### 多项式回归法
* 多项式回归，回归函数是回归变量多项式的回归。多项式回归模型是线性回归模型的一种，此时回归函数关于回归系数是线性的。由于任一函数都可以用多项式逼近，因此多项式回归有着广泛应用。
直线回归研究的是一个因变量与一个自变量之间的回归问题，但在实际情况中，影响因变量的自变量往往不止一个，例如：羊毛的产量受到绵羊体重、体长、胸围等影响，因此需要进行一个因变量与多个自变量间的回归分析，即多元回归分析。
研究一个因变量与一个或多个自变量间多项式的回归分析方法，称为多项式回归（Polynomial Regression）。如果自变量只有一个时，称为一元多项式回归；如果自变量有多个时，称为多元多项式回归。在一元回归分析中，如果依变量y与自变量x的关系为非线性的，但是又找不到适当的函数曲线来拟合，则可以采用一元多项式回归。
* 二次多项式训练过程与结果

|损失函数值|拟合结果|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_loss_2p.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_result_2p.png">|
从表的损失函数曲线上看，没有任何损失值下降的趋势；再看拟合情况，只拟合成了一条直线。这说明二次多项式不能满足要求。以下是最后几行的打印输出：

```
......
9989 49 0.09410913779071385
9999 49 0.09628814270449357
W= [[-1.72915813]
 [-0.16961507]]
B= [[0.98611283]]
```
#### step4小结
简单来说，激活函数的作用就是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。在神经网络中，隐藏层之间的输出大多需要通过激活函数来映射，在构建模型时，需要根据实际数据情况选择激活函数。TensorFlow中的激活函数不止这4种，这里只是介绍最常用的4个。对于多层神经网络主要完成的任务是拟合而不是分类。所以输出层不用激活函数（Sigmoid既是激活函数，又是分类函数，是个特例），通过这一条特性我们可以做到明确的区分激活函数和分类函数。
简言之：
神经网络最后一层不需要激活函数
激活函数只用于连接前后两层神经网络 （非常重要）

#### step4心得体会
在多层神经网络模型中，不论是回归/拟合问题还是分类问题，最后一层均不需要激活函数，需要激活函数的都是前面几层，最后一层如果做分类，则会添加分类函数如sigmoid/softmax，如果是拟合/回归问题则什么都不需要添加。
千万不要拖作业，一定要合理安排自己的时间，这样才能高效高质量完成作业。以外通过温习，我对激活函数有了更深的理解与感悟。

>>>>>>>## step5 简要回顾总结
#### 非线性分类
在本部分中，我们将学习更复杂的分类问题，比如，在很多年前，两位著名的学者证明了感知机无法解决逻辑中的异或问题，从而使感知机这个研究领域陷入了长期的停滞。我们将会在使用双层网络解决异或问题。
异或问题是个简单的二分类问题，因为毕竟只有4个样本数据，我们会用更复杂的数据样本来学习非线性多分类问题，并理解其工作原理。
然后我们将会用一个稍微复杂些的二分类例子，来说明在二维平面上，神经网络是通过怎样的神奇的线性变换加激活函数压缩，把线性不可分的问题转化为线性可分问题的。
解决完二分类问题，我们将学习如何解决更复杂的三分类问题，由于样本的复杂性，必须在隐层使用多个神经元才能完成分类任务。
最后我们将搭建一个三层神经网络，来解决MNIST手写数字识别问题，并学习使用梯度检查来帮助我们测试反向传播代码的正确性。
数据集的使用，是深度学习的一个基本技能，开发集、验证集、测试集，合理地使用才能得到理想的泛化能力强的模型。

##### 非线性二分类
* 逻辑异或门
  [![Dg9qKI.png](https://s3.ax1x.com/2020/11/29/Dg9qKI.png)](https://imgchr.com/i/Dg9qKI)
  理想分类结果
  [![Dg9xIS.png](https://s3.ax1x.com/2020/11/29/Dg9xIS.png)](https://imgchr.com/i/Dg9xIS)
* 我们把异或问题归类成二分类问题，所以使用二分类交叉熵损失函数：
[![DgCGdK.png](https://s3.ax1x.com/2020/11/29/DgCGdK.png)](https://imgchr.com/i/DgCGdK)
* 隐层神经元数量的选择
一般来说，隐层的神经元数量要大于等于输入特征的数量，在本例中是2。我们从下图可以看到，如果隐层只有一个神经元的话，是不能完成分类任务的。
[![Dgk7u9.png](https://s3.ax1x.com/2020/11/29/Dgk7u9.png)](https://imgchr.com/i/Dgk7u9)
* 完成学习的过程
损失函数值的变化与分类效果对比
[![DgAPHI.png](https://s3.ax1x.com/2020/11/29/DgAPHI.png)](https://imgchr.com/i/DgAPHI)
二分类模型的评估标准

* 准确率 Accuracy

也可以称之为精度，我们在本书中混用这两个词。

对于二分类问题，假设测试集上一共1000个样本，其中550个正例，450个负例。测试一个模型时，得到的结果是：521个正例样本被判断为正类，435个负例样本被判断为负类，则正确率计算如下：

$$Accuracy=(521+435)/1000=0.956$$

即正确率为95.6%。这种方式对多分类也是有效的，即三类中判别正确的样本数除以总样本数，即为准确率。

但是这种计算方法丢失了很多细节，比如：是正类判断的精度高还是负类判断的精度高呢？因此，我们还有如下一种评估标准。
*  混淆矩阵
如果具体深入到每个类别上，会分成4部分来评估：

- 正例中被判断为正类的样本数（TP-True Positive）：521
- 正例中被判断为负类的样本数（FN-False Negative）：550-521=29
- 负例中被判断为负类的样本数（TN-True Negative）：435
- 负例中被判断为正类的样本数（FP-False Positive）：450-435=15

* 我们覆盖了父类中的三个方法：
- `init()` 初始化方法：因为父类的初始化方法要求有两个参数，代表train/test数据文件
- `ReadData()`方法：父类方法是直接读取数据文件，此处直接在内存中生成样本数据，并且直接令训练集等于原始数据集（不需要归一化），令测试集等于训练集
- `GenerateValidationSet()`方法，由于只有4个样本，所以直接令验证集等于训练集

因为`NeuralNet2`中的代码要求数据集比较全，有训练集、验证集、测试集，为了已有代码能顺利跑通，我们把验证集、测试集都设置成与训练集一致，对于解决这个异或问题没有什么影响。

```Python
class XOR_DataReader(DataReader):
    def ReadData(self):
        self.XTrainRaw = np.array([0,0,0,1,1,0,1,1]).reshape(4,2)
        self.YTrainRaw = np.array([0,1,1,0]).reshape(4,1)
        self.XTrain = self.XTrainRaw
        self.YTrain = self.YTrainRaw
        self.num_category = 1
        self.num_train = self.XTrainRaw.shape[0]
        self.num_feature = self.XTrainRaw.shape[1]
        self.XTestRaw = self.XTrainRaw
        self.YTestRaw = self.YTrainRaw
        self.XTest = self.XTestRaw
        self.YTest = self.YTestRaw
        self.num_test = self.num_train

    def GenerateValidationSet(self, k = 10):
        self.XVld = self.XTrain
        self.YVld = self.YTrain
```

* 测试函数

与第6章中的逻辑与门和或门一样，我们需要神经网络的运算结果达到一定的精度，也就是非常的接近0，1两端，而不是说勉强大于0.5就近似为1了，所以精度要求是误差绝对值小于`1e-2`。

```Python
def Test(dataReader, net):
    print("testing...")
    X,Y = dataReader.GetTestSet()
    A = net.inference(X)
    diff = np.abs(A-Y)
    result = np.where(diff < 1e-2, True, False)
    if result.sum() == dataReader.num_test:
        return True
    else:
        return False
```

* 主过程代码

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden = 2
    n_output = 1
    eta, batch_size, max_epoch = 0.1, 1, 10000
    eps = 0.005
    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.BinaryClassifier, InitialMethod.Xavier)
    net = NeuralNet2(hp, "Xor_221")
    net.train(dataReader, 100, True)
    ......
```

此处的代码有几个需要强调的细节：

- `n_input = dataReader.num_feature`，值为2，而且必须为2，因为只有两个特征值
- `n_hidden=2`，这是人为设置的隐层神经元数量，可以是大于2的任何整数
- `eps`精度=0.005是后验知识，笔者通过测试得到的停止条件，用于方便案例讲解
- 网络类型是`NetType.BinaryClassifier`，指明是二分类网络
- 最后要调用`Test`函数验证精度

* 运行结果

经过快速的迭代后，会显示训练过程如图10-10所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_loss.png" />

图10-10 训练过程中的损失函数值和准确率值的变化

可以看到二者的走势很理想。

同时在控制台会打印一些信息，最后几行如下：

```
......
epoch=5799, total_iteration=23199
loss_train=0.005553, accuracy_train=1.000000
loss_valid=0.005058, accuracy_valid=1.000000
epoch=5899, total_iteration=23599
loss_train=0.005438, accuracy_train=1.000000
loss_valid=0.004952, accuracy_valid=1.000000
W= [[-7.10166559  5.48008579]
 [-7.10286572  5.48050039]]
B= [[ 2.91305831 -8.48569781]]
W= [[-12.06031599]
 [-12.26898815]]
B= [[5.97067802]]
testing...
1.0
None
testing...
A2= [[0.00418973]
 [0.99457721]
 [0.99457729]
 [0.00474491]]
True
```
一共用了5900个`epoch`，达到了指定的`loss`精度（0.005），`loss_valid`是0.004991，刚好小于0.005时停止迭代。

我们特意打印出了`A2`值，即网络推理结果，如表10-7所示。

表10-7 异或计算值与神经网络推理值的比较

|x1|x2|XOR|Inference|diff|
|---|---|---|---|---|
|0|0|0|0.0041|0.0041|
|0|1|1|0.9945|0.0055|
|1|0|1|0.9945|0.0055|
|1|1|0|0.0047|0.0047|
#### 非线性多分类
多入多出的双层神经网络 - 非线性多分类

* 多分类模型的评估标准

我们以三分类问题举例，假设每类有100个样本，一共300个样本，最后的分类结果如表所示。

表 多分类结果的混淆矩阵

|样本所属类别|分到类1|分到类2|分到类3|各类样本总数|精(准)确率|
|---|---|---|---|---|---|
|类1|90|4|6|100|90%|
|类2|9|84|5|100|84%|
|类3|1|4|95|100|95%|
|总数|101|93|106|300|89.67%|

- 第1类样本，被错分到2类4个，错分到3类6个，正确90个；
- 第2类样本，被错分到1类9个，错分到3类5个，正确84个；
- 第3类样本，被错分到1类1个，错分到2类4个，正确95个；
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/nn.png" />

 非线性多分类的神经网络结构图
 - 输入层两个特征值$x_1, x_2$
$$
x=
\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
$$
- 隐层$2\times 3$的权重矩阵$W1$
$$
W1=
\begin{pmatrix}
    w1_{11} & w1_{12} & w1_{13} \\\\
    w1_{21} & w1_{22} & w1_{23}
\end{pmatrix}
$$

- 隐层$1\times 3$的偏移矩阵$B1$

$$
B1=\begin{pmatrix}
    b1_1 & b1_2 & b1_3 
\end{pmatrix}
$$

- 隐层由3个神经元构成
- 输出层$3\times 3$的权重矩阵$W2$
$$
W2=\begin{pmatrix}
    w2_{11} & w2_{12} & w2_{13} \\\\
    w2_{21} & w2_{22} & w2_{23} \\\\
    w2_{31} & w2_{32} & w2_{33} 
\end{pmatrix}
$$

- 输出层$1\times 1$的偏移矩阵$B2$

$$
B2=\begin{pmatrix}
    b2_1 & b2_2 & b2_3 
  \end{pmatrix}
$$

- 输出层有3个神经元使用Softmax函数进行分类

* 前向计算

根据网络结构，可以绘制前向计算图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_forward.png" />

* 第一层

- 线性计算

$$
z1_1 = x_1 w1_{11} + x_2 w1_{21} + b1_1
$$
$$
z1_2 = x_1 w1_{12} + x_2 w1_{22} + b1_2
$$
$$
z1_3 = x_1 w1_{13} + x_2 w1_{23} + b1_3
$$
$$
Z1 = X \cdot W1 + B1
$$

- 激活函数

$$
a1_1 = Sigmoid(z1_1) 
$$
$$
a1_2 = Sigmoid(z1_2) 
$$
$$
a1_3 = Sigmoid(z1_3) 
$$
$$
A1 = Sigmoid(Z1)
$$

* 第二层

- 线性计算

$$
z2_1 = a1_1 w2_{11} + a1_2 w2_{21} + a1_3 w2_{31} + b2_1
$$
$$
z2_2 = a1_1 w2_{12} + a1_2 w2_{22} + a1_3 w2_{32} + b2_2
$$
$$
z2_3 = a1_1 w2_{13} + a1_2 w2_{23} + a1_3 w2_{33} + b2_3
$$
$$
Z2 = A1 \cdot W2 + B2
$$

- 分类函数

$$
a2_1 = \frac{e^{z2_1}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_2 = \frac{e^{z2_2}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_3 = \frac{e^{z2_3}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
A2 = Softmax(Z2)
$$

* 损失函数

使用多分类交叉熵损失函数：
$$
loss = -(y_1 \ln a2_1 + y_2 \ln a2_2 + y_3 \ln a2_3)
$$
$$
J(w,b) = -\frac{1}{m} \sum^m_{i=1} \sum^n_{j=1} y_{ij} \ln (a2_{ij})
$$

$m$为样本数，$n$为类别数。
* 代码实现

绝大部分代码都在`HelperClass2`目录中的基本类实现，这里只有主过程：

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden = 3
    n_output = dataReader.num_category
    eta, batch_size, max_epoch = 0.1, 10, 5000
    eps = 0.1
    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.MultipleClassifier, InitialMethod.Xavier)
    # create net and train
    net = NeuralNet2(hp, "Bank_233")
    net.train(dataReader, 100, True)
    net.ShowTrainingTrace()
    # show result
    ......
```

过程描述：

1. 读取数据文件
2. 显示原始数据样本分布图
3. 其它数据操作：归一化、打乱顺序、建立验证集
4. 设置超参
5. 建立神经网络开始训练
6. 显示训练结果
* 多入多出的三层神经网络 - 深度非线性多分类
*  三层神经网络的实现
* 定义神经网络

为了完成MNIST分类，我们需要设计一个三层神经网络结构，如图12-2所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/nn3.png" ch="500" />

三层神经网络结构

* 输入层

共计$28\times 28=784$个特征值：

$$
X=\begin{pmatrix}
    x_1 & x_2 & \cdots & x_{784}
  \end{pmatrix}
$$

* 隐层1

- 权重矩阵$W1$形状为$784\times 64$

$$
W1=\begin{pmatrix}
    w1_{1,1} & w1_{1,2} & \cdots & w1_{1,64} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w1_{784,1} & w1_{784,2} & \cdots & w1_{784,64} 
  \end{pmatrix}
$$

- 偏移矩阵$B1$的形状为$1\times 64$

$$
B1=\begin{pmatrix}
    b1_{1} & b1_{2} & \cdots & b1_{64}
  \end{pmatrix}
$$

- 隐层1由64个神经元构成，其结果为$1\times 64$的矩阵

$$
Z1=\begin{pmatrix}
    z1_{1} & z1_{2} & \cdots & z1_{64}
  \end{pmatrix}
$$
$$
A1=\begin{pmatrix}
    a1_{1} & a1_{2} & \cdots & a1_{64}
  \end{pmatrix}
$$

* 隐层2

- 权重矩阵$w2$形状为$64\times 16$

$$
W2=\begin{pmatrix}
    w2_{1,1} & w2_{1,2} & \cdots & w2_{1,16} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w2_{64,1} & w2_{64,2} & \cdots & w2_{64,16} 
  \end{pmatrix}
$$

- 偏移矩阵#B2#的形状是$1\times 16$

$$
B2=\begin{pmatrix}
    b2_{1} & b2_{2} & \cdots & b2_{16}
  \end{pmatrix}
$$

- 隐层2由16个神经元构成

$$
Z2=\begin{pmatrix}
    z2_{1} & z2_{2} & \cdots & z2_{16}
  \end{pmatrix}
$$
$$
A2=\begin{pmatrix}
    a2_{1} & a2_{2} & \cdots & a2_{16}
  \end{pmatrix}
$$

* 输出层

- 权重矩阵$W3$的形状为$16\times 10$

$$
W3=\begin{pmatrix}
    w3_{1,1} & w3_{1,2} & \cdots & w3_{1,10} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w3_{16,1} & w3_{16,2} & \cdots & w3_{16,10} 
  \end{pmatrix}
$$

- 输出层的偏移矩阵$B3$的形状是$1\times 10$

$$
B3=\begin{pmatrix}
    b3_{1}& b3_{2} & \cdots & b3_{10}
  \end{pmatrix}
$$

- 输出层有10个神经元使用Softmax函数进行分类

$$
Z3=\begin{pmatrix}
    z3_{1} & z3_{2} & \cdots & z3_{10}
  \end{pmatrix}
$$
$$
A3=\begin{pmatrix}
    a3_{1} & a3_{2} & \cdots & a3_{10}
  \end{pmatrix}
$$
*  前向计算

我们都是用大写符号的矩阵形式的公式来描述，在每个矩阵符号的右上角是其形状。

* 隐层1

$$Z1 = X \cdot W1 + B1 \tag{1}$$

$$A1 = Sigmoid(Z1) \tag{2}$$

* 隐层2

$$Z2 = A1 \cdot W2 + B2 \tag{3}$$

$$A2 = Tanh(Z2) \tag{4}$$

* 输出层

$$Z3 = A2 \cdot W3  + B3 \tag{5}$$

$$A3 = Softmax(Z3) \tag{6}$$

我们的约定是行为样本，列为一个样本的所有特征，这里是784个特征，因为图片高和宽均为28，总共784个点，把每一个点的值做为特征向量。

两个隐层，分别定义64个神经元和16个神经元。第一个隐层用Sigmoid激活函数，第二个隐层用Tanh激活函数。

输出层10个神经元，再加上一个Softmax计算，最后有$a1,a2,...a10$共十个输出，分别代表0-9的10个数字。

*  反向传播

和以前的两层网络没有多大区别，只不过多了一层，而且用了tanh激活函数，目的是想把更多的梯度值回传，因为tanh函数比sigmoid函数稍微好一些，比如原点对称，零点梯度值大。

* 输出层

$$dZ3 = A3-Y \tag{7}$$
$$dW3 = A2^{\top} \cdot dZ3 \tag{8}$$
$$dB3=dZ3 \tag{9}$$

* 隐层2

$$dA2 = dZ3 \cdot W3^{\top} \tag{10}$$
$$dZ2 = dA2 \odot (1-A2 \odot A2) \tag{11}$$
$$dW2 = A1^{\top} \cdot dZ2 \tag{12}$$
$$dB2 = dZ2 \tag{13}$$

* 隐层1

$$dA1 = dZ2 \cdot W2^{\top} \tag{14}$$
$$dZ1 = dA1 \odot A1 \odot (1-A1) \tag{15}$$
$$dW1 = X^{\top} \cdot dZ1 \tag{16}$$
$$dB1 = dZ1 \tag{17}$$
*  梯度检查


神经网络算法使用反向传播计算目标函数关于每个参数的梯度，可以看做解析梯度。由于计算过程中涉及到的参数很多，用代码实现的反向传播计算的梯度很容易出现误差，导致最后迭代得到效果很差的参数值。

为了确认代码中反向传播计算的梯度是否正确，可以采用梯度检验（gradient check）的方法。通过计算数值梯度，得到梯度的近似值，然后和反向传播得到的梯度进行比较，若两者相差很小的话则证明反向传播的代码是正确无误的。

* 泰勒公式

泰勒公式是将一个在$x=x_0$处具有n阶导数的函数$f(x)$利用关于$(x-x_0)$的n次多项式来逼近函数的方法。若函数$f(x)$在包含$x_0$的某个闭区间$[a,b]$上具有n阶导数，且在开区间$(a,b)$上具有$n+1$阶导数，则对闭区间$[a,b]$上任意一点$x$，下式成立：

$$f(x)=\frac{f(x_0)}{0!} + \frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2 + ...+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x) \tag{3}$$

其中,$f^{(n)}(x)$表示$f(x)$的$n$阶导数，等号后的多项式称为函数$f(x)$在$x_0$处的泰勒展开式，剩余的$R_n(x)$是泰勒公式的余项，是$(x-x_0)^n$的高阶无穷小。 

利用泰勒展开公式，令$x=\theta + h, x_0=\theta$，我们可以得到：

$$f(\theta + h)=f(\theta) + f'(\theta)h + O(h^2) \tag{4}$$

* 单边逼近误差

如果用单边逼近，把公式4两边除以$h$后变形：

$$f'(\theta) + O(h)=\frac{f(\theta+h)-f(\theta)}{h} \tag{5}$$

公式5已经和公式1的定义非常接近了，只是左侧多出来的第二项，就是逼近的误差，是个$O(h)$级别的误差项。

* 双边逼近误差

如果用双边逼近，我们用三阶泰勒展开：

令$x=\theta + h, x_0=\theta$，我们可以得到：

$$f(\theta + h)=f(\theta) + f'(\theta)h + f''(\theta)h^2 + O(h^3) \tag{6}$$

再令$x=\theta - h, x_0=\theta$我们可以得到：

$$f(\theta - h)=f(\theta) - f'(\theta)h + f''(\theta)h^2 - O(h^3) \tag{7}$$

公式6减去公式7，有：

$$f(\theta + h) - f(\theta - h)=2f'(\theta)h + 2O(h^3) \tag{8}$$

两边除以$2h$：

$$f'(\theta) + O(h^2)={f(\theta + h) - f(\theta - h) \over 2h} \tag{9}$$
*  学习率与批大小

在梯度下降公式中：

$$
w_{t+1} = w_t - \frac{\eta}{m} \sum_i^m \nabla J(w,b) \tag{1}
$$

其中，$\eta$是学习率，m是批大小。所以，学习率与批大小是对梯度下降影响最大的两个因子。
* 初始学习率的选择

我们前面一直使用固定的学习率，比如0.1或者0.05，而没有采用0.5、0.8这样高的学习率。这是因为在接近极小点时，损失函数的梯度也会变小，使用小的学习率时，不会担心步子太大越过极小点。

保证SGD收敛的充分条件是：

$$\sum_{k=1}^\infty \eta_k = \infty \tag{2}$$

且： 

$$\sum_{k=1}^\infty \eta^2_k < \infty \tag{3}$$ 

图是不同的学习率的选择对训练结果的影响。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/learning_rate.png" ch="500" />

图 学习率对训练的影响

- 黄色：学习率太大，loss值增高，网络发散
- 红色：学习率可以使网络收敛，但值较大，开始时loss值下降很快，但到达极值点附近时，在最优解附近来回跳跃
- 绿色：正确的学习率设置
- 蓝色：学习率值太小，loss值下降速度慢，训练次数长，收敛慢

有一种方式可以帮助我们快速找到合适的初始学习率。

Leslie N. Smith 在2015年的一篇论文[Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)中的描述了一个非常棒的方法来找初始学习率。

这个方法在论文中是用来估计网络允许的最小学习率和最大学习率，我们也可以用来找我们的最优初始学习率，方法非常简单：

1. 首先我们设置一个非常小的初始学习率，比如`1e-5`；
2. 然后在每个`batch`之后都更新网络，计算损失函数值，同时增加学习率；
3. 最后我们可以描绘出学习率的变化曲线和loss的变化曲线，从中就能够发现最好的学习率。

表就是随着迭代次数的增加，学习率不断增加的曲线，以及不同的学习率对应的loss的曲线（理想中的曲线）。

表 试验最佳学习率

|随着迭代次数增加学习率|观察Loss值与学习率的关系|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\12\lr-select-1.jpg">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\12\lr-select-2.jpg">|

从表的右图可以看到，学习率在0.3左右表现最好，再大就有可能发散了。我们把这个方法用于到我们的代码中试一下是否有效。

首先，设计一个数据结构，做出表。

表 学习率与迭代次数试验设计

|学习率段|0.0001~0.0009|0.001~0.009|0.01~0.09|0.1~0.9|1.0~1.1|
|----|----|----|----|---|---|
|步长|0.0001|0.001|0.01|0.1|0.01|
|迭代|10|10|10|10|10|

对于每个学习率段，在每个点上迭代10次，然后：

$$当前学习率+步长 \rightarrow 下一个学习率$$

以第一段为例，会在0.1迭代100次，在0.2上迭代100次，......，在0.9上迭代100次。步长和迭代次数可以分段设置，得到图。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_1.png" ch="500" />

 第一轮的学习率测试

横坐标用了`np.log10()`函数来显示对数值，所以横坐标与学习率的对应关系如表所示。

表横坐标与学习率的对应关系

|横坐标|-1.0|-0.8|-0.6|-0.4|-0.2|0.0|
|--|--|--|--|--|--|--|
|学习率|0.1|0.16|0.25|0.4|0.62|1.0|

前面一大段都是在下降，说明学习率为0.1、0.16、0.25、0.4时都太小了，那我们就继续探查-0.4后的段，得到第二轮测试结果如图。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_2.png" ch="500" />

 第二轮的学习率测试

到-0.13时（对应学习率0.74）开始，损失值上升，所以合理的初始学习率应该是0.7左右，于是我们再次把范围缩小的0.6，0.7，0.8去做试验，得到第三轮测试结果，如图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_3.png" ch="500" />

图12-8 第三轮的学习率测试

最后得到的最佳初始学习率是0.8左右。由于loss值是渐渐从下降变为上升的，前面有一个积累的过程，如果想避免由于前几轮迭代带来的影响，可以使用比0.8小一些的数值，比如0.75作为初始学习率。

* 学习率的后期修正

固定批大小为128时，我们分别使用学习率为0.2，0.3，0.5，0.8来比较一下学习曲线。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/acc_bs_128.png" ch="500" />

 不同学习率对应的迭代次数与准确度值的

学习率为0.5时效果最好，虽然0.8的学习率开始时上升得很快，但是到了10个`epoch`时，0.5的曲线就超上来了，最后稳定在0.8的曲线之上。

这就给了我们一个提示：可以在开始时，把学习率设置大一些，让准确率快速上升，损失值快速下降；到了一定阶段后，可以换用小一些的学习率继续训练。用公式表示：

$$
LR_{new}=LR_{current} * DecayRate^{GlobalStep/DecaySteps} \tag{4}
$$

举例来说：

- 当前的LR = 0.1
- DecayRate = 0.9
- DecaySteps = 50

公式变为：

$$lr = 0.1 * 0.9^{GlobalSteps/50}$$

意思是初始学习率为0.1，每训练50轮计算一次新的$lr$，是当前的$0.9^n$倍，其中$n$是正整数，因为一般用$GlobalSteps/50$的结果取整，所以$n=1,2,3,\ldots$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/lr_decay.png" ch="500" />

阶梯状学习率下降法

如果计算一下每50轮的衰减的具体数值，见表。

表 学习率衰减值计算

|迭代|0|50|100|150|200|250|300|...|
|---|---|---|---|---|---|---|---|---|
|学习率|0.1|0.09|0.081|0.073|0.065|0.059|0.053|...|

这样的话，在开始时可以快速收敛，到后来变得很谨慎，小心翼翼地向极值点逼近，避免由于步子过大而跳过去。

上面描述的算法叫做step算法，还有一些其他的算法如下。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/lr_policy.png" ch="500" />

 其他各种学习率下降算法

* fixed

使用固定的学习率，比如全程都用0.1。要注意的是，这个值不能大，否则在后期接近极值点时不易收敛。

* step

每迭代一个预订的次数后（比如500步），就调低一次学习率。离散型，简单实用。

* multistep

预设几个迭代次数，到达后调低学习率。与step不同的是，这里的次数可以是不均匀的，比如3000、5500、8000。离散型，简单实用。

* exp

连续的指数变化的学习率，公式为：

$$lr_{new}=lr_{base} * \gamma^{iteration} \tag{5}$$

由于一般的iteration都很大（训练需要很多次迭代），所以学习率衰减得很快。$\gamma$可以取值0.9、0.99等接近于1的数值，数值越大，学习率的衰减越慢。

* inv

倒数型变化，公式为：

$$lr_{new}=lr_{base} * \frac{1}{( 1 + \gamma * iteration)^{p}} \tag{6}$$

$\gamma$控制下降速率，取值越大下降速率越快；$p$控制最小极限值，取值越大时最小值越小，可以用0.5来做缺省值。

* poly

多项式衰减，公式为：

$$lr_{new}=lr_{base} * (1 - {iteration \over iteration_{max}})^p \tag{7}$$

$p=1$时，为线性下降；$p>1$时，下降趋势向上突起；$p<1$时，下降趋势向下凹陷。$p$可以设置为0.9。



#### step5小结
非线性判别函数解决比较复杂的线性不可分样本分类问题，解决问题比较简便的方法是采用多个线性分界面将它们分段连接，用分段线性判别划分去逼近分界的超曲面。
• 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归。
  优缺点：算法简单和具有“学习”能力。线性分类器速度快、编程方便，但是可能拟合效  果不会很好。
• 常见的非线性分类器：决策树、RF、GBDT、多层感知机。
  优缺点：非线性分类器编程复杂，但是效果拟合能力强。
#### step5心得体会
在异或问题中，我知道了如果使用三维坐标系来分析平面上任意复杂的分类问题，都可以迎刃而解。

>>>>>>> ## step6 回顾小结
#### 模型的推理与部署
* 训练是通过从已有的数据中学习到某种能力，而推理是简化并使用该能力，使其能快速、高效地对未知的数据进行操作，以获得预期的结果。
[![DgQWpn.png](https://s3.ax1x.com/2020/11/29/DgQWpn.png)](https://imgchr.com/i/DgQWpn)
训练是计算密集型操作，模型一般都需要使用大量的数据来进行训练，通过反向传播来不断的优化模型的参数，以使得模型获取某种能力。在训练的过程中，我们常常是将模型在数据集上面的拟合情况放在首要位置的。而推理过程在很多场景下，除了模型的精度外，还更加关注模型的大小和速度等指标。这就需要对训练的模型进行一些压缩、剪枝或者是操作上面的计算优化。
* 重要指标
Throughput 吞吐量
单位时间内所处理的数据量 一般用 推理/秒 或者 样本/秒 衡量。每台服务器的吞吐量对于数据中心能否合算的扩展至关重要。
* 包含并行情况 
def calc_ips(batch_size, time): 
		# 全局进程个数
    world_size = (
        torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1
    )
    tbs = world_size * batch_size
    return tbs / time

Latency 延迟
执行一次推理所花的时间，单位一般为ms。低延迟对于实时且快速增长地推理服务至关重要。一般在压测时，我们都是通过增加并发数，来观察 Latency 平均线、90线、 95线和99线
 time_list 为每个请求从发送到返回的时间列表
avg = np.mean(time_list)
cf_90 = max(time_list[:int(len(time_list) * 0.90)])
cf_95 = max(time_list[:int(len(time_list) * 0.95)])
cf_99 = max(time_list[:int(len(time_list) * 0.99)])
Accuracy 准确率
训练后的模型能够提供正确结果的能力。一般推理时，我们会评估模型在模型压缩或者优化后能够和训练时达到一样或者可接受的相似效果。模型是否正确部署，结果具有幂等性。同时根据应用场景的情况，我们可以针对自己在意的指标进行衡量（和训练时相同）。
Memory usage 内存使用情况
在众多场景下，在推理过程中很关注内存的使用情况。尤其是在多个网络模型并且内存资源有限的系统中尤为重要。另外，有时也需要在意内存的利用率情况，这对于评估资源是否浪费以及模型外工程方面的优化方向至关重要。
 Memory usage 内存使用情况
在众多场景下，在推理过程中很关注内存的使用情况。尤其是在多个网络模型并且内存资源有限的系统中尤为重要。另外，有时也需要在意内存的利用率情况，这对于评估资源是否浪费以及模型外工程方面的优化方向至关重要。
Efficiency 效率
单位功率的吞吐量， 一般单位为 performance/watt。Efficiency是数据中心扩展合算分析的另一个关键因素。因为服务器、服务器机架和整个数据中心必须在固定的功率预算内运行。

FLOPs
是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。

FLOPS
是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。

#### step6小结
根据实际场景进行优化和变型，如果要在此之上要追求极致的性能，就只能自己动手丰衣足食了，例如用C++或Go重写推理部分，或者利用大公司自研的框架进行部署和推理。
#### step6心得体会
开源服务已经将服务通信、模型管理等都做好了，我们也可以在这些开源项目上面进行应用和二次开发。值得一提的话，如果你真的希望模型服务性能上有很大的改善，要针对性的进行优化工作，寻找瓶颈在哪里。如果需要模型压缩和优化的就需要对模型的结构有一定的了解

>>>>>> ## step 7 简要回顾与总结
### 搭建深度神经网络框架
   [![DgGVPg.png](https://s3.ax1x.com/2020/11/29/DgGVPg.png)](https://imgchr.com/i/DgGVPg)
1. NeuralNet:来包装基本的神经网络结构和功能
    + Layers：神经网络各层的容器，按添加顺序维护一个列表。
    + Parameters：基本参数（包括普参和超参）
    + Loss Function：提供计算损失函数值，存储历史记录并最后绘图的功能
    + LayerManagement()：添加神经网络层
    + ForwardCalculation():调用各层的前向计算方法
    + BackPropagation()：调用各层的反向传播方法
    + PreUpdateWeights() ：预更新各层的权重参数
    + UpdateWeights()：更新各层的权重参数
    + Train()：训练
    + SaveWeights()：保存各层的权重参数
    + LoadWeights()：加载各层的权重参数
2. Layer：是一个抽象类，以及更加需要增加的实际类
    + 包括：
        + Fully Connected Layer
        + Classification Layer
        + Activator Layer
        + Dropout Layer
        + Batch Norm Layer
        + Convolution Layer（将来会包括）
        + Max Pool Layer（将来会包括）
    + 每个Layer都包括以下基本方法：
        + ForwardCalculation():调用各层的前向计算方法
        + BackPropagation()：调用各层的反向传播方法
        + PreUpdateWeights() ：预更新各层的权重参数
        + UpdateWeights()：更新各层的权重参数
        + SaveWeights()：保存各层的权重参数
        + LoadWeights()：加载各层的权重参数
3. Activator Layer：激活函数和分类函数
    + Identity：直传函数，即没有激活处理
    + Sigmoid：S型函数，也成S型生长曲线。由于其单增以及反函数单增等性质，在信息科学中，Sigmoid函数常被用作神经网络的阈值函数。
    + Tanh:双曲函数中的一个，tanh()双曲正切。
        + 公式：tanh(x)=2*sigmoid(2*x)-1和![](2.png)
    + Relu：线性整流函数，又称修正线性单元。
4. Classification Layer：分类函数
    + Sigmoid二分类
    + Softmax多分类
5. Parameters：基本神经网络运行参数：
    + 学习率
    + 最大epoch
    + batch size
    + 损失函数定义
    + 初始化方法
    + 优化器类型
    + 停止条件
    + 正则类型和条件
6. LossFunction：损失函数及帮助方法（之前有学过）
    + 均方差函数
    + 交叉熵函数二分类
    + 交叉熵函数多分类
    + 记录损失函数
    + 显示损失函数历史记录
    + 获得最小函数值时的权重参数
7. Optimizer：优化器
    + SGD：随机梯度下降。以单个样本为训练单元训练速度会很快，但牺牲了向量化运算所带来的便利性，在较大数据集上效率并不高。
    + Momentum：带动量的梯度下降。带动量的梯度下降考虑历史梯度的加权平均值作为速率进行优化。
        + 执行公式：![](3.png) ![](4.png) 
    + Nag:
    + AdaGrad
    + AdaDelta
    + RMSProp
    + Adam:是在带动量的梯度下降法的基础上融合了一种称为 RMSprop（加速梯度下降）的算法而成的。
        + 计算公式：![](5.png) 
8. WeightsBias:权重矩阵，仅供全连接层使用
   + 初始化：
        + Zero, Normal, MSRA (HE), Xavier
        + 保存初始化值
        + 加载初始化值
    + Pre_Update：预更新
    + Update：更新
    + Save：保存训练结果值
    + Load：加载训练结果值
9.  DataReader 样本数据读取器
    + ReadData：从文件中读取数据
    + NormalizeX：归一化样本值
    + NormalizeY：归一化标签值
    + GetBatchSamples：获得批数据
    + ToOneHot：标签值变成OneHot编码用于多分类
    + ToZeorOne：标签值变成0/1编码用于二分类
    + Shuffle：打乱样本顺序
    + MnistImageDataReader：读取MNIST数据（从中派生的数据读取器）
    + CifarImageReader：读取Cifar10数据（从中派生的数据读取器）

### 回归测试-万能近似定理
1. 搭建模型：一个双层的神经网络，第一层后面接一个Sigmoid激活函数，第二层直接输出拟合数据。[![DgGmxs.png](https://s3.ax1x.com/2020/11/29/DgGmxs.png)](https://imgchr.com/i/DgGmxs)
2. 超参数说明：
    + 输入层1个神经元，因为只有一个x值
    + 隐层4个神经元，对于此问题来说应该是足够了，因为特征很少
    + 输出层1个神经元，因为是拟合任务
    + 学习率=0.5
    + 最大epoch=10000轮
    + 批量样本数=10
    + 拟合网络类型
    + Xavier初始化
    + 绝对损失停止条件=0.001 
3. 训练结果：代码测试：
    + [![DgGlZV.png](https://s3.ax1x.com/2020/11/29/DgGlZV.png)](https://imgchr.com/i/DgGlZV)

### 反向传播四大公式推导
+ 著名的反向传播四大公式：
     + $$\delta^{L} = \nabla_{a}C \odot \sigma_{'}(Z^L) \tag{80}$$ $$\delta^{l} = ((W^{l + 1})^T\delta^{l+1})\odot\sigma_{'}(Z^l) \tag{81}$$ $$\frac{\partial{C}}{\partial{b_j^l}} = \delta_j^l \tag{82}$$ $$\frac{\partial{C}}{\partial{w_{jk}^{l}}} = a_k^{l-1}\delta_j^l \tag{83}$$
+ 这个也挺难的，参考书是：矩阵求导术

### 回归任务 - 房价预测
+ 数据处理：
    + 原始数据只有一个数据集，需要自己把它分成训练集和测试集，比例大概为4:1。此数据集为csv文件格式，为了方便，我们把它转换成了两个扩展名为npz的numpy压缩形式：
        + house_Train.npz，训练数据集
        + house_Test.npz，测试数据集 
+ 搭建模型：
    +  个模型包含了四组全连接层-Relu层的组合，最后是一个单输出做拟合。
    +  超参数说明：
        + 学习率=0.1
        + 最大epoch=1000
        + 批大小=16
        + 拟合网络
        + 初始化方法Xavier
        + 停止条件为相对误差1e-7
        + net.train()函数是一个阻塞函数，只有当训练完毕后才返回
+ 代码运行结果：
    +[![DgG3IU.png](https://s3.ax1x.com/2020/11/29/DgG3IU.png)](https://imgchr.com/i/DgG3IU)


### 二分类试验 - 双弧形非线性二分类
+ 搭建模型：同样是一个双层神经网络，但是最后一层要接一个Logistic二分类函数来完成二分类任务。
+ 超参数说明：
    + 输入层神经元数为2
    + 隐层的神经元数为3，使用Sigmoid激活函数
    + 由于是二分类任务，所以输出层只有一个神经元，用Logistic做二分类函数
    + 最多训练1000轮
    + 批大小=5
    + 学习率=0.1
    + 绝对误差停止条件=0.02
+ 代码运行结果：
    + [![DgJtk8.png](https://s3.ax1x.com/2020/11/29/DgJtk8.png)](https://imgchr.com/i/DgJtk8)


### 二分类任务 - 居民收入
+ 字段解读：年龄、工作性质、权重、教育程度、受教育时长、婚姻状况、职业、家庭角色、性别、资本收益、资本损失、每周工作时长、祖籍
+ 数据处理：对于连续值，我们可以直接使用原始数据。对于枚举型，我们需要把它们转成连续值。以性别举例，Female=0，Male=1即可。对于其它枚举型，都可以用从0开始的整数编码。
+ 搭建模型：搭建一个与房价预测一样的网络结构，不同的是为了完成二分类任务，在最后接一个Logistic函数。
+ 超参数说明：
    + 学习率=0.1
    + 最大epoch=100
    + 批大小=16
    + 二分类网络类型
    + MSRA初始化
    + 相对误差停止条件1e-3
    + net.train()是一个阻塞函数，只有当训练完毕后才返回
+ 训练结果：
    + [![DgJdpQ.png](https://s3.ax1x.com/2020/11/29/DgJdpQ.png)](https://imgchr.com/i/DgJdpQ)
### 多分类功能测试 - “铜钱孔分类”问题
+ 搭建模型
    + 模型一：
        + 模型：使用Sigmoid做为激活函数的两层网络
        + 超参数说明
            + 隐层8个神经元
            + 最大epoch=5000
            + 批大小=10
            + 学习率0.1
            + 绝对误差停止条件=0.08
            + 多分类网络类型
            + 初始化方法为Xavier
            + net.train()函数是一个阻塞函数，只有当训练完毕后才返回。
    + 模型二：
        + 模型：使用Relu做为激活函数的三层网络。 ![](13.png)
        + 超参数说明
            + 隐层8个神经元
            + 最大epoch=5000
            + 批大小=10
            + 学习率0.1
            + 绝对误差停止条件=0.08
            + 多分类网络类型
            + 初始化方法为MSRA
+ 运行结果：
    +[![DgJznI.png](https://s3.ax1x.com/2020/11/29/DgJznI.png)](https://imgchr.com/i/DgJznI)

### 多分类任务 - MNIST手写体识别
+ 搭建模型：一共4个隐层，都用Relu()激活函数连接，最后的输出层接Softmax分类函数
+ 运行结果：
    + [![DgYCAf.png](https://s3.ax1x.com/2020/11/29/DgYCAf.png)](https://imgchr.com/i/DgYCAf)
  
#### step7 小结
深度神经网络（Deep Neural Networks）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型
#### step7 心得体会
理解DNN反向传播算法的前提就是理解DNN的模型和前向传播算法。

>>>>>>>>>>>>> ## 手写数字体识别
>>>>>>>>>>> ### 2.启动UWP
  #### 从GitHub下载项目后，启动Visual Studio并打开MNIST_Demo.sln文件。如
  * InkCanvas绘制数字。用于解释数字并清除画布的按钮。
  *  MainPage.xaml-我们所有的XAML代码都为InkCanvas，按钮和标签创建UI 。
  *  MainPage.xaml.cs-我们的应用程序代码所在的位置。
  *  Helper.cs-裁剪和转换图像格式的帮助程序例程.
  
#### 构建并运行项目
 在Visual Studio工具栏中，将解决方案平台更改为x64。
 要运行项目，请单击工具栏上的“ 开始调试”按钮，或按F5键。该应用程序应该显示一个InkCanvas，用户可以在其中写一个数字，一个Recognize按钮来解释该数字，一个空标签字段，其中解释后的数字将以文本形式显示，以及一个Clear Digit按钮来清除InkCanvas。

#### 添加模型
右键单击解决方案资源管理器中的Assets文件夹，然后添加现有项，选择ONNX模型的位置，然后单击添加。该项目现在应该有两个新文件： mnist.onnx-训练的模型。 mnist.cs -Windows ML生成的代码。为了确保在编译应用程序时能够构建模型，请右键单击mnist.onnx文件，然后选择Properties。对于Build Action，选择Content。
#### 分为三类的mnist.cs文件中的新生成的代码：
  * MNISTModel创建机器学习模型：在系统默认设备上创建会话，将特定的输入和输出绑定到模型，并一步评估模型。
  * mnistInput初始化模型期望的输入类型：在这种情况下，输入需要一个ImageFeatureValue。
  * mnistOutput初始化模型将输出的类型：在这种情况下，输出将是TensorFloat类型的名为Plus214_Output_0的列表。

#### 加载，绑定和评估模型
  * 对于Windows ML应用程序，我们要遵循的模式是："加载">"绑定">"求值"
  * 加载机器学习模型：将输入和输出绑定到模型。评估模型并查看结果。
### 
 1. 在MainPage.xaml.cs中，我们实例化模型，输入和输出。
 2. 在LoadModelAsync中，我们将加载模型：用之前那个方法MainPage的加载事件，在OnNavigateTo覆盖。该MNISTModle类表示MNIST模式并创建系统默认设备上的会话。要加载模型，我们调用CreateFromStreamAsync方法，并传入ONNX文件作为参数
 3. 
    + 我们将要输入和输出绑定到模型：生成的代码还包括MNISTInput（该模型的预期输入）和mnistOutput（该模型的预期输出）包装器类。（该mnistInput类期待一个ImageFeatureValue，所以我们使用一个辅助方法获取ImageFeatureValue为输入。）
    + 使用helper.cs中包含的帮助函数，我们将复制InkCanvas的内容，将其转换为ImageFeatureValue类型，并将其绑定到我们的模型。
    + 对于输出：我们只需使用指定的输入调用EvaluateAsync。输入初始化后，调用模型的EvaluateAsync方法一根据输入数据评估模型。EvaluateAsync将输入和输出绑定到模型对象并在输入上评估模型。
    + 由于模型返回了输出张量，因此我们首先要将其转换为友好的数据类型，然后解析返回的列表以确定哪个数字具有最高的概率并显示该数字。
 4. 我们要清除InkCanvas，以允许用户绘制另一个数字。
#### 运行结果
  [![Dg7dC6.png](https://s3.ax1x.com/2020/11/30/Dg7dC6.png)](https://imgchr.com/i/Dg7dC6)

>>>>### 总结 
  #### 遇到的问题很多。这个比之前的做的都比之前的难一些，虽然有老师提供的代码但还是中途出了不少错误。通过问同学，他们帮我解决的。在自己检查的时候总觉得没有出错，但其实漏洞很多~。一定要细心啊，不然推翻重来真的好麻烦。。。













