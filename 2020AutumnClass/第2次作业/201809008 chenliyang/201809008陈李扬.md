# Step2

## **一、算法简介**

### **1.1 什么是回归分析**

回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线/线来拟合数据点，目标是使曲线到数据点的距离差异最小。

### **1.2 线性回归**

线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数$w$和$b$。通长我们可以表达成如下公式：

$$\widehat{y}=wx+b$$

$\widehat{y}$为预测值，自变量$x$和因变量$y$是已知的，而我们想实现的是预测新增一个$x$，其对应的$y$是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中$w$和$b$两个参数。

### **1.3 目标/损失函数**

求解最佳参数，需要一个标准来对结果进行衡量，为此我们需要定量化一个目标函数式，使得计算机可以在求解过程中不断地优化。

针对任何模型求解问题，都是最终都是可以得到一组预测值$\widehat{y}$，对比已有的真实值 $y$ ，数据行数为 $n$ ，可以将损失函数定义如下：

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225403953-1601026207.png" width="30%" />
</div>

即预测值与真实值之间的平均的平方距离，统计中一般称其为**MAE(mean square error)均方误差**。把之前的函数式代入损失函数，并且将需要求解的参数$w$和$b$看做是函数$L$的自变量，可得

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225443234-242375165.png" width="40%" />
</div>

现在的任务是求解最小化$L$时$w$和$b$的值，
即核心目标优化式为

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225522497-461663601.png" width="40%" />
</div>

求解方式有：
>**1）最小二乘法(least square method)**
求解 $w$ 和 $b$ 是使损失函数最小化的过程，在统计中，称为**线性回归模型的最小二乘“参数估计”(parameter estimation)**。

我们可以将 $L(w,b)$ 分别对 $w$ 和 $b$ 求导，得到

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225511784-1188061678.png" width="40%" />
</div>

令上述两式为$0$，可得到 $w$ 和 $b$ **最优解的闭式(closed-form)解**：

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225639407-822453102.png" width="40%" />
</div>

>**2）梯度下降(gradient descent)**

梯度下降核心内容是对自变量进行不断的更新（针对w和b求偏导），使得目标函数不断逼近最小值的过程

<div align=center>
<img src="https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225705480-236089.png" width="30%" />
</div>

>**3)神经网络法** 
### 1. **定义神经网络结构**

我们是首次尝试建立神经网络，先用一个最简单的单层单点神经元，如图4-4所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/Setup.png" ch="500" />

图4-4 单层单点神经元

#### **输入层**

此神经元在输入层只接受一个输入特征，经过参数 $w,b$ 的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。

严格来说输入层在神经网络中并不能称为一个层。

#### **权重 $w,b$**

因为是一元线性问题，所以 $w,b$ 都是标量。

#### **输出层**

输出层 $1$ 个神经元，线性预测公式是：

$$z_i = x_i \cdot w + b$$

$z$ 是模型的预测输出，$y$ 是实际的样本标签值，下标 $i$ 为样本。

#### **损失函数**

因为是线性回归问题，所以损失函数使用均方差函数。

$$loss(w,b) = \frac{1}{2} (z_i-y_i)^2$$

### 2.**反向传播**

由于我们使用了和上一节中的梯度下降法同样的数学原理，所以反向传播的算法也是一样的，细节请查看4.2.2。

#### **计算 $w$ 的梯度**

$$
{\partial{loss} \over \partial{w}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i
$$

#### **计算 $b$ 的梯度**

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i
$$

>**4)多样本计算** 

### **前向计算**

<div align=center>
<img src="https://img-blog.csdnimg.cn/img_convert/07e9efff73ad4e8cf528305497236cdf.png" ch="500" />
</div>

举个例子，假设上一层结点$i,j,k,…$等一些结点与本层的结点$w$有连接，那么结点$w$的值怎么算呢？就是通过上一层的$i,j,k$等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如$ReLu，sigmoid$等函数，最后得到的结果就是本层结点$w$的输出。最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：

$$a^2=\sigma(aW^2+b^2)$$

其中，上标代表层数，星号表示卷积，$b$表示偏置项$bias$，$\sigma$表示激活函数。

>**5)正规方程解法**

英文名是 Normal Equations。

对于线性回归问题，除了前面提到的最小二乘法可以解决一元线性回归的问题外，也可以解决多元线性回归问题。

对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。它可以解决下面这个公式描述的问题：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k \tag{1}$$

### **1.简单的推导方法**

在做函数拟合（回归）时，我们假设函数 $H$ 为：

$$H(w,b) = b + x_1 w_1+x_2 w_2+ \dots +x_n w_n \tag{2}$$

令 $b=w_0$，则：

$$H(W) = w_0 + x_1 \cdot w_1 + x_2 \cdot w_2 + \dots + x_n \cdot w_n\tag{3}$$

公式3中的 $x$ 是一个样本的 $n$ 个特征值，如果我们把 $m$ 个样本一起计算，将会得到下面这个矩阵：

$$H(W) = X \cdot W \tag{4}$$

公式5中的 $X$ 和 $W$ 的矩阵形状如下：

$$
X = 
\begin{pmatrix} 
1 & x_{1,1} & x_{1,2} & \dots & x_{1,n} \\\\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,n} \\\\
\vdots & \vdots & \vdots & \ddots & \vdots \\\\
1 & x_{m,1} & x_{m,2} & \dots & x_{m,n}
\end{pmatrix} \tag{5}
$$

$$
W= \begin{pmatrix}
w_0 \\\\
w_1 \\\\
\vdots \\\\
 w_n
\end{pmatrix}  \tag{6}
$$

然后我们期望假设函数的输出与真实值一致，则有：

$$H(W) = X \cdot W = Y \tag{7}$$

其中，Y的形状如下：

$$
Y= \begin{pmatrix}
y_1 \\\\
y_2 \\\\
\vdots \\\\
y_m
\end{pmatrix}  \tag{8}
$$


直观上看，$W = Y/X$，但是这里三个值都是矩阵，而矩阵没有除法，所以需要得到 $X$ 的逆矩阵，用 $Y$ 乘以 $X$ 的逆矩阵即可。但是又会遇到一个问题，只有方阵才有逆矩阵，而 $X$ 不一定是方阵，所以要先把左侧变成方阵，就可能会有逆矩阵存在了。所以，先把等式两边同时乘以 $X$ 的转置矩阵，以便得到 $X$ 的方阵：

$$X^{\top} X W = X^{\top} Y \tag{9}$$

其中，$X^{\top}$ 是 $X$ 的转置矩阵，$X^{\top}X$ 一定是个方阵，并且假设其存在逆矩阵，把它移到等式右侧来：

$$W = (X^{\top} X)^{-1}{X^{\top} Y} \tag{10}$$

至此可以求出 $W$ 的正规方程。

## **二、数据标准化（Normalization）**

### **为什么要做归一化**

理论层面上，神经网络是以**样本在事件中的统计分布概率为基础**进行训练和预测的，也就是说：

>1.**样本的各个特征的取值要符合概率分布**，即$[0,1]$

>2.**样本的度量单位要相同**。我们并没有办法去比较$1$米和$1$公斤的区别，但是，如果我们知道了$1$米在整个样本中的大小比例，以及$1$公斤在整个样本中的大小比例，比如一个处于$0.2$的比例位置，另一个处于$0.3$的比例位置，就可以说这个样本的$1$米比$1$公斤要小！

>3.**神经网络**:假设所有的输入输出数据都是标准差为$1$，均值为$0$，包括权重值的初始化，激活函数的选择，以及优化算法的的设计。

>4.**数值问题**：归一化/标准化可以避免一些不必要的数值问题。因为**sigmoid/tanh**的非线性区间大约在$[-1.7，1.7]$。意味着要使神经元有效，$tanh( w_1x_1 + w_2x_2 +b)$ 里的 $w_1x_1 +w_2x_2 +b$ 数量级应该在 $1$ （$1.7$所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。

>5.**初始化**：在初始化时我们希望每个神经元初始化成有效的状态，**tansig函数**在$[-1.7, 1.7]$范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在$[-1,1]$且输入没有归一化且过大，会使得神经元饱和）

>6.**梯度**：以**输入-隐层-输出**这样的三层**BP**为例，我们知道对于**输入-隐层**权值的梯度有$2ew(1-a^2)\cdot x$的形式（$e$是误差，$w$是隐层到输出层的权重，$a$是隐层神经元的值，$x$是输入），若果输出层的数量级很大，会引起$e$的数量级很大，同理，$w$为了将隐层（数量级为$1$）映身到输出层，$w$也会很大，再加上$x$也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。

>7.**学习率**：知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 对$w_1$适合的学习率，可能相对于$w_2$来说会太小，若果使用适合$w_1$的学习率，会导致在$w_2$方向上步进非常慢，会消耗非常多的时间，而使用适合$w_2$的学习率，对$w_1$来说又太大，搜索不到适合$w_1$的解。如果使用固定学习率，而数据没归一化，则后果可想而知。

### **标准化的常用方法**


- Min-Max标准化（离差标准化），将数据映射到 $[0,1]$ 区间

$$x_{new}=\frac{x-x_{min}}{x_{max} - x_{min}} \tag{1}$$

- 平均值标准化，将数据映射到[-1,1]区间
   
$$x_{new} = \frac{x - \bar{x}}{x_{max} - x_{min}} \tag{2}$$

- 对数转换
$$x_{new}=\ln(x_i) \tag{3}$$

- 反正切转换
$$x_{new}=\frac{2}{\pi}\arctan(x_i) \tag{4}$$

- Z-Score法

把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。Z-Score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：

$$x_{new} = \frac{x_i - \bar{x}}{std} \tag{5}$$

- 中心化，平均值为0，无标准差要求
  
$$x_{new} = x_i - \bar{x} \tag{6}$$

- 比例法，要求数据全是正值

$$
x_{new} = \frac{x_k}{\sum_{i=1}^m{x_i}} \tag{7}
$$

# Step3

## **线性分类**

### **1.二分类函数**
对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。而在很多不太正规的文字材料中，把这两个概念混用了，比如下面这个说法：“我们在最后使用$Sigmoid$激活函数来做二分类”，这是不恰当的。在本书中，我们会根据不同的任务区分激活函数和分类函数这两个概念，在二分类任务中，叫做$Logistic$函数，而在作为激活函数时，叫做$Sigmoid$函数。

- Logistic函数公式

$$Logistic(z) = \frac{1}{1 + e^{-z}}$$

以下记 $a=Logistic(z)$。

- 导数

$$Logistic'(z) = a(1 - a)$$

具体求导过程可以参考8.1节。

- 输入值域

$$(-\infty, \infty)$$

- 输出值域

$$(0,1)$$

- 函数图像（图6-2）

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/logistic.png" ch="500" />

图6-2 $Logistic$函数图像

#### **二分类的代数原理**

代数方式：通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5。

### **2.多分类函数**

$softmax$函数，又称归一化指数函数。它是二分类函数$sigmoid$在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。下图展示了$softmax$的计算方法：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20181128162309759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6X3BldGVy,size_16,color_FFFFFF,t_70" ch="500" />
</div>

#### **为什么$softmax$是这种形式?**

概率有两个性质：
>1）预测的概率为非负数；

下图为$y=exp(x）$的图像，我们可以知道**指数函数**的值域取值范围是零到正无穷。$softmax$第一步就是将模型的预测结果转化到指数函数上，这样保证了概率的非负性。

<div align=center>
<img src="https://img-blog.csdnimg.cn/2019111910482712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6X3BldGVy,size_16,color_FFFFFF,t_70" ch="500" />
</div>

>2）各种预测结果概率之和等于1。

为了确保各个预测结果的概率之和等于1。我们只需要将转换后的结果进行归一化处理。方法就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比。这样就得到近似的概率。

**$softmax$**就是将在**负无穷到正无穷**上的预测结果按照这两步转换为概率的。

下面为大家举一个例子，假如模型对一个三分类问题的预测结果为$-3、1.5、2.7$。我们要用$softmax$将模型结果转为概率。步骤如下：

1）将预测结果转化为非负数

$y_1 = exp(x1) = exp(-3) = 0.05$

$y_2 = exp(x2) = exp(1.5) = 4.48$

$y_3 = exp(x3) = exp(2.7) = 14.88$

2）各种预测结果概率之和等于1

$z_1 = y_1/(y_1+y_2+y_3) = 0.05/(0.05+4.48+14.88) = 0.0026$

$z_2 = y_2/(y_1+y_2+y_3) = 4.48/(0.05+4.48+14.88) = 0.2308$

$z_3 = y_3/(y_1+y_2+y_3) = 14.88/(0.05+4.48+14.88) = 0.7666$

总结一下$softmax$如何将多分类输出转换为概率，可以分为两步：

1）分子：通过指数函数，将实数输出映射到零到正无穷。

2）分母：将所有结果相加，进行归一化。