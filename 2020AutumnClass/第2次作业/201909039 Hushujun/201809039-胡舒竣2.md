# 线性分类

神经网络的一个重要功能就是分类，现实世界中的分类任务复杂多样，但万变不离其宗，我们都可以用同一种模式的神经网络来处理。

本部分中，我们从最简单的线性二分类开始学习，包括其原理，实现，训练过程，推理过程等等，并且以可视化的方式来帮助大家更好地理解这些过程。

在本部分中，我们将利用学到的二分类知识，实现逻辑与门、与非门，或门，或非门。

## 线性二分类

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_data.png" width="400" />
|样本序号|$X_1$:经度相对值|$X_2$:纬度相对值|$Y$:1=汉, 0=楚|
|---|---|---|---|
|1|0.325|0.888|1|
|2|0.656|0.629|0|
|3|0.151|0.101|1|
|4|0.785|0.024|0|
|...|...|...|...|
|200|0.631|0.001|0|

我们在上一章学习了特征归一化的方法。在本例中，中原地区的经纬度坐标其实应该是一个两位数以上的实数，比如 $(35.234, -122.455)$。为了简化问题，我们已经把它们归一化到 $[0,1]$ 之间了。

问题：

1. 经纬度相对坐标值为 $(0.58,0.92)$ 时，属于楚还是汉？
2. 经纬度相对坐标值为 $(0.62,0.55)$ 时，属于楚还是汉？
3. 经纬度相对坐标值为 $(0.39,0.29)$ 时，属于楚还是汉？

读者可能会觉得这个太简单了，这不是有图吗？定位坐标值后在图上一比划，一下子就能找到对应的区域了。但是我们要求用机器学习的方法来解决这个看似简单的问题，以便将来的预测行为是快速准确的，而不是拿个尺子在图上去比划。



## 逻辑回归模型
回归问题可以分为两类：
线性回归使用一条直线拟合样本数据，而逻辑回归的目标是“拟合”0或1两个数值，而不是具体连续数值，所以称为广义线性模型。逻辑回归又称Logistic回归分析，常用于数据挖掘，疾病自动诊断，经济预测等领域。
逻辑回归的另外一个名字叫做分类器，分为线性分类器和非线性分类器，本章中我们学习线性分类器。而无论是线性还是非线性分类器，又分为两种：二分类问题和多分类问题，在本章中我们学习二分类问题。线性多分类问题将会在下一章讲述，非线性分类问题在后续的步骤中讲述。
|线性二分类|非线性二分类|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_binary.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/non_linear_binary.png"/>|
# 定义神经网络结构

根据前面的猜测，看来我们只需要一个二入一出的神经元就可以搞定。这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络，见图6-3。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/BinaryClassifierNN.png" ch="500" />

完成二分类任务的神经元结构

与上一章的网络结构图的区别是，这次我们在神经元输出时使用了分类函数，所以输出为 $A$，而不是以往直接输出的 $Z$。
## 线性二分类原理

||线性回归|线性分类|
|---|---|---|
|相同点|需要在样本群中找到一条直线|需要在样本群中找到一条直线|
|不同点|用直线来拟合所有样本，使得各个样本到这条直线的距离尽可能最短|用直线来分割所有样本，使得正例样本和负例样本尽可能分布在直线两侧|

可以看到线性回归中的目标--“距离最短”，还是很容易理解的，但是线性分类的目标--“分布在两侧”，用数学方式如何描述呢？我们可以有代数和几何两种方式来描述。

# 实现逻辑与或非门

单层神经网络，又叫做感知机，它可以轻松实现逻辑与、或、非门。由于逻辑与、或门，需要有两个变量输入，而逻辑非门只有一个变量输入。但是它们共同的特点是输入为0或1，可以看作是正负两个类别。

- 与门 AND
- 与非门 NAND
- 或门 OR
- 或非门 NOR
- 非门 NOT
 
以逻辑AND为例，图6-12中的4个点分别代表4个样本数据，蓝色圆点表示负例（$y=0$），红色三角表示正例（$y=1$）。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/LogicAndGateData.png" ch="500" />

## 多分类函数定义 - Softmax

#### 如何得到多分类问题的分类结果概率值？

Logistic函数可以得到诸如0.8、0.3这样的二分类概率值，前者接近1，后者接近0。那么多分类问题如何得到类似的概率值呢？

我们依然假设对于一个样本的分类值是用这个线性公式得到的：

$$
z = x \cdot w + b
$$

但是，我们要求 $z$ 不是一个标量，而是一个向量。如果是三分类问题，我们就要求 $z$ 是一个三维的向量，向量中的每个单元的元素值代表该样本分别属于三个分类的值，这样不就可以了吗？

具体的说，假设$x$是一个 (1x2) 的向量，把w设计成一个(2x3)的向量，b设计成(1x3)的向量，则z就是一个(1x3)的向量。我们假设z的计算结果是$[3,1,-3]$，这三个值分别代表了样本x在三个分类中的数值，下面我们把它转换成概率值。

### 7.2.3 代码实现

绝大部分代码可以从上一章的`HelperClass`中拷贝出来，但是需要我们为了本章的特殊需要稍加修改。

#### 添加分类函数

在`Activators.py`中，增加Softmax的实现，并添加单元测试。

```Python
class Softmax(object):
    def forward(self, z):
        ......
```

#### 前向计算

前向计算需要增加分类函数调用：

```Python
class NeuralNet(object):
    def forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Logistic().forward(Z)
            return A
        elif self.params.net_type == NetType.MultipleClassifier:
            A = Softmax().forward(Z)
            return A
        else:
            return Z
```

#### 反向传播

在多分类函数一节详细介绍了反向传播的推导过程，推导的结果很令人惊喜，就是一个简单的减法，与前面学习的拟合、二分类的算法结果都一样。

```Python
class NeuralNet(object):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        ......
```

#### 计算损失函数值

损失函数不再是均方差和二分类交叉熵了，而是交叉熵函数对于多分类的形式，并且添加条件分支来判断只在网络类型为多分类时调用此损失函数。

```Python
class LossFunction(object):
    # fcFunc: feed forward calculation
    def CheckLoss(self, A, Y):
        m = Y.shape[0]
        if self.net_type == NetType.Fitting:
            loss = self.MSE(A, Y, m)
        elif self.net_type == NetType.BinaryClassifier:
            loss = self.CE2(A, Y, m)
        elif self.net_type == NetType.MultipleClassifier:
            loss = self.CE3(A, Y, m)
        #end if
        return loss
    # end def

    # for multiple classifier
    def CE3(self, A, Y, count):
        ......
```

#### 推理函数

```Python
def inference(net, reader):
    xt_raw = np.array([5,1,7,6,5,6,2,7]).reshape(4,2)
    xt = reader.NormalizePredicateData(xt_raw)
    output = net.inference(xt)
    r = np.argmax(output, axis=1)+1
    print("output=", output)
    print("r=", r)
```
注意在推理之前，先做了归一化，因为原始数据是在[0,10]范围的。

函数`np.argmax`的作用是比较`output`里面的几个数据的值，返回最大的那个数据的行数或者列数，0-based。比如output=(1.02,-3,2.2)时，会返回2，因为2.2最大，所以我们再加1，把返回值变成1,2,3的其中一个。

`np.argmax`函数的参数`axis=1`，是因为有4个样本参与预测，所以需要在第二维上区分开来，分别计算每个样本的argmax值。

#### 主程序

```Python
if __name__ == '__main__':
    num_category = 3
    ......
    num_input = 2
    params = HyperParameters(num_input, num_category, eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.MultipleClassifier)
    ......
```

### 运行结果

#### 损失函数历史记录

从图7-8所示的趋势上来看，损失函数值还有进一步下降的可能，以提高模型精度。有兴趣的读者可以多训练几轮，看看效果。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/loss.png" ch="500" />

图7-8 训练过程中损失函数值的变化

下面是打印输出的最后几行：

```
......
epoch=99
99 13 0.25497053433985734
W= [[-1.43234109 -3.57409342  5.00643451]
 [ 4.47791288 -2.88936887 -1.58854401]]
B= [[-1.81896724  3.66606162 -1.84709438]]
output= [[0.01801124 0.73435241 0.24763634]
 [0.24709055 0.15438074 0.59852871]
 [0.38304995 0.37347646 0.24347359]
 [0.51360269 0.46266935 0.02372795]]
r= [2 3 1 1]
```
注意，r是分类预测结果，对于每个测试样本的结果，是按行看的，即第一行是第一个测试样本的分类结果。

1. 经纬度相对值为 $(5,1)$ 时，概率0.734最大，属于2，蜀国
2. 经纬度相对值为 $(7,6)$ 时，概率0.598最大，属于3，吴国
3. 经纬度相对值为 $(5,6)$ 时，概率0.383最大，属于1，魏国
4. 经纬度相对值为 $(2,7)$ 时，概率0.513最大，属于1，魏国

# 多分类结果可视化

神经网络到底是一对一方式，还是一对多方式呢？从Softmax公式，好像是一对多方式，因为只取一个最大值，那么理想中的一对多方式应该是图7-13所示的样子。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/OneVsOthers.png" ch="500" />
