# 【Step2】

>>>># 04 线性回归


>>### 04.0 人工智能发展简史

#### 单入单出的单层神经网络 - 单变量线性回归

在互联网建设初期，各大运营商需要解决的问题就是保证服务器所在的机房的温度常年保持在23摄氏度左右。在一个新建的机房里，如果计划部署346台服务器，我们如何配置空调的最大功率？

这个问题虽然能通过热力学计算得到公式，但是总会有误差。因此人们往往会在机房里装一个温控器，来控制空调的开关或者风扇的转速或者制冷能力，其中最大制冷能力是一个关键性的数值。更先进的做法是直接把机房建在海底，用隔离的海水循环降低空气温度的方式来冷却。

#### 一元线性回归模型

回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。

最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：

$$Y=a+bX+\varepsilon \tag{1}$$

$X$ 是自变量，$Y$ 是因变量，$\varepsilon$ 是随机误差，$a$ 和 $b$ 是参数，在线性回归模型中，$a,b$ 是我们要通过算法学习出来的。

什么叫模型？第一次接触这个概念时，可能会有些不明觉厉。从常规概念上讲，是人们通过主观意识借助实体或者虚拟表现来构成对客观事物的描述，这种描述通常是有一定的逻辑或者数学含义的抽象表达方式。

比如对小轿车建模的话，会是这样描述：由发动机驱动的四轮铁壳子。对能量概念建模的话，那就是爱因斯坦狭义相对论的著名推论：$E=mc^2$。

对数据建模的话，就是想办法用一个或几个公式来描述这些数据的产生条件或者相互关系，比如有一组数据是大致满足 $y=3x+2$ 这个公式的，那么这个公式就是模型。为什么说是“大致”呢？因为在现实世界中，一般都有噪音（误差）存在，所以不可能非常准确地满足这个公式，只要是在这条直线两侧附近，就可以算作是满足条件。

>>### 04.1 最小二乘法

### 4.1.2 数学原理

线性回归试图学得：

$$z_i=w \cdot x_i+b \tag{1}$$

使得：

$$z_i \simeq y_i \tag{2}$$

其中，$x_i$ 是样本特征值，$y_i$ 是样本标签值，$z_i$ 是模型预测值。

如何学得 $w$ 和 $b$ 呢？均方差(MSE - mean squared error)是回归任务中常用的手段：
$$
J = \frac{1}{2m}\sum_{i=1}^m(z_i-y_i)^2 = \frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2 \tag{3}
$$

$J$ 称为损失函数。实际上就是试图找到一条直线，使所有样本到直线上的残差的平方和最小。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/4/mse.png" />

图4-3 均方差函数的评估原理

图4-3中，圆形点是样本点，直线是当前的拟合结果。如左图所示，我们是要计算样本点到直线的垂直距离，需要再根据直线的斜率来求垂足然后再计算距离，这样计算起来很慢；但实际上，在工程上我们通常使用的是右图的方式，即样本点到直线的竖直距离，因为这样计算很方便，用一个减法就可以了。

>>### 04.2 梯度下降法

#### 数学原理

在下面的公式中，我们规定 $x$ 是样本特征值（单特征），$y$ 是样本标签值，$z$ 是预测值，下标 $i$ 表示其中一个样本。

#### 预设函数（Hypothesis Function）

线性函数：

$$z_i = x_i \cdot w + b \tag{1}$$

#### 损失函数（Loss Function）

均方误差：

$$loss_i(w,b) = \frac{1}{2} (z_i-y_i)^2 \tag{2}$$


与最小二乘法比较可以看到，梯度下降法和最小二乘法的模型及损失函数是相同的，都是一个线性模型加均方差损失函数，模型用于拟合，损失函数用于评估效果

>>### 04.3 神经网络法

神经网络做线性拟合的原理：

1. 初始化权重值
2. 根据权重值放出一个解
3. 根据均方差函数求误差
4. 误差反向传播给线性计算部分以调整权重值
5. 是否满足终止条件？不满足的话跳回2

>>### 04.4 多样本计算

单样本计算有一些缺点：

1. 前后两个相邻的样本很有可能会对反向传播产生相反的作用而互相抵消。假设样本1造成了误差为 $0.5$，$w$ 的梯度计算结果是 $0.1$；紧接着样本2造成的误差为 $-0.5$，$w$ 的梯度计算结果是 $-0.1$，那么前后两次更新 $w$ 就会产生互相抵消的作用。
2. 在样本数据量大时，逐个计算会花费很长的时间。由于我们在本例中样本量不大（200个样本），所以计算速度很快，觉察不到这一点。在实际的工程实践中，动辄10万甚至100万的数据量，轮询一次要花费很长的时间。

>>>># 05 多入单出的单层神经网络 - 多变量线性回归

>>### 05.0 多变量线性回归问题
反向传播，梯度下降，损失函数。

神经网络训练的最基本的思想就是：先“猜”一个结果，称为预测结果 $a$，看看这个预测结果和事先标记好的训练集中的真实结果 $y$ 之间的差距，然后调整策略，再试一次，这一次就不是“猜”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即 $|a-y|\rightarrow 0$，就结束训练。

####反向传播与梯度下降的基本工作原理：

1.初始化；
2.正向计算；
3.损失函数为我们提供了计算损失的方法；
4.梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重5.调整的方向；
6.反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值7.反向调整权重；
8.Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。

>>### 05.1 正规方程法

对于线性回归问题，除了前面提到的最小二乘法可以解决一元线性回归的问题外，也可以解决多元线性回归问题。

对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。它可以解决下面这个公式描述的问题：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k \tag{1}$$


>>### 05.2 神经网络法

#### 定义神经网络结构

我们定义一个如图5-1所示的一层的神经网络，输入层为2或者更多，反正大于2了就没区别。这个一层的神经网络的特点是：

1. 没有中间层，只有输入项和输出层（输入项不算做一层）；
2. 输出层只有一个神经元；
3. 神经元有一个线性输出，不经过激活函数处理，即在下图中，经过 $\Sigma$ 求和得到 $Z$ 值之后，直接把 $Z$ 值输出。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/5/setup.png" ch="500" />

图5-1 多入单出的单层神经元结构

与上一章的神经元相比，这次仅仅是多了一个输入，但却是质的变化，即，一个神经元可以同时接收多个输入，这是神经网络能够处理复杂逻辑的根本。

>>### 05.3 样本特征数据标准化

数据标准化（Normalization），又可以叫做数据归一化。

#### 发现问题的根源

仔细分析一下屏幕打印信息，前两次迭代的损失值已经是天文数字了，后面的W和B的值也在不断变大，说明网络发散了。难道我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。

回想一个问题：为什么在第4章中，我们没有遇到这种情况？把第4章的数据样本拿来看一看，如表5-4所示。

表5-4 第4章中的样本数据

|样本序号|服务器数量(千台)X|空调功率(千瓦)Y|
|---|---|---|
|1|0.928|4.824|
|2|0.469|2.950|
|3|0.855|4.643|
|...|...|...|

所有的X值（服务器数量除以1000后的值）都是在 $[0,1]$ 之间的，而本章中的房价数据有两个特征值，一个是公里数，一个是平米数，全都不在 $[0,1]$ 之间，且取值范围不相同。我们不妨把本次样本数据也做一下这样的处理，亦即“标准化”。

其实，数据标准化是深度学习的必要步骤之一，已经是大师们的必杀技能，也因此它很少被各种博客/文章所提及，以至于初学者们经常被坑。

根据5.0.1中对数据的初步统计，我们是不是也可以把公里数都除以100，而平米数都除以1000呢？这样可能也会得到 $[0,1]$ 之间的数字？公里数的取值范围是 $[2,22]$，除以100后变成了 $[0.02,0.22]$。平米数的取值范围是 $[40,120]$，除以1000后变成了 $[0.04,0.12]$。

对本例来说这样做肯定是可以正常工作的，但是下面我们要介绍一种更科学合理的做法。

>>### 05.4 还原参数值

### 5.4.1 对比结果

在上一节中，我们使用了如下超参进行神经网络的训练：

```Python
params = HyperParameters(eta=0.1, max_epoch=10, batch_size=1, eps = 1e-5)
```

我们再把每次checkpoint的`W`和`B`的值打印出来：

```
9 0 437.5399553941636 [[-35.46926435] [399.01136072]] [[252.69305588]]
9 100 420.78580862641473 [[-36.93198181] [400.03047293]] [[251.26503706]]
......
9 900 413.7210407763991 [[-36.67601742] [406.55322285]] [[246.8067483]]
```
打印结果中每列的含义按顺序是：`epoch`,`iteration`,`loss`,`w1`,`w2`,`b`。

可以看到`loss`,`w1`,`w2`,`b`的值，每次跳跃都很大，怀疑是学习率过高导致的梯度下降在最优解附近徘徊。所以，我们先把超参修改一下：

```Python
params = HyperParameters(eta=0.01, max_epoch=500, batch_size=10, eps=1e-5)
```

>>### 05.6 对标签值标准化

我们既然已经对样本数据特征值做了标准化，那么如此大数值的损失函数值是怎么来的呢？看一看损失函数定义：

$$
J(w,b)=\frac{1}{2m} \sum_{i=1}^m (z_i-y_i)^2 \tag{1}
$$

其中，$z_i$ 是预测值，$y_i$ 是标签值。初始状态时，$W$ 和 $B$ 都是 $0$，所以，经过前向计算函数 $Z=X \cdot W+B$ 的结果是 $0$，但是 $Y$ 值很大，处于 $[181.38, 674.37]$ 之间，再经过平方计算后，一下子就成为至少5位数的数值了。

再看反向传播时的过程：

```Python
    def __backwardBatch(self, batch_x, batch_y, batch_z):
        m = batch_x.shape[0]
        dZ = batch_z - batch_y
        dB = dZ.sum(axis=0, keepdims=True)/m
        dW = np.dot(batch_x.T, dZ)/m
        return dW, dB
```
第二行代码求得的`dZ`，与房价是同一数量级的，这样经过反向传播后，`dW`和`dB`的值也会很大，导致整个反向传播链的数值都很大。我们可以debug一下，得到第一反向传播时的数值是：
```
dW
array([[-142.59982906],
       [-283.62409678]])
dB
array([[-443.04543906]])
```
上述数值又可能在读者的机器上是不一样的，因为样本做了shuffle，但是不影响我们对问题的分析。


>>>># 学习总结
通过学习step2,对线性回归有了具体细致的了解和学习，不得不说如果想更加深入的学习并把知识运用到实践中，还需要花很大的功夫。