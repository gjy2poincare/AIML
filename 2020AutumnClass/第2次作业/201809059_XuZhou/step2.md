# 最小二乘法

**要求**：`利用直接计算的方式求得渐进曲线，但要保证样本特征有逆矩阵`

**原理**:利用导数计算使得所有离散点到所要求直线的纵距离最小

计算公式

$$
w = \frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x^2_i - (\sum_{i=1}^m x_i)^2/m} \tag{15}
$$

`b值可以由原式求出`

```Python
def methd(X,Y,m):
#X,Y都是样本特征矩阵，m为样本个数。
    x_mean = X.mean()    
"""
mean是numpy的函数
若A为矩阵，则输出每一列的均值（一个向量）
若A为列向量，则输出均值（一个数）
若A为行向量，则也是输出均值（一个数），和列向量一样
"""
    p = sum(Y*(X-x_mean))
#sum是numpy的函数，元素求和
    q = sum(X**2)-(sum(X)**2)/m
    w = p/q
```

# 梯度下降法

**要求**：`先随机的确定权值和偏置，计算出损失函数，然后再通过梯度下降的办法，在反向修改权重，让损失函数达到最小值`
**原理**：学习率的大小需要自己进行定义

```python
#当损失函数形式为均值平方差时
def __mothd(X,Y,w,b,eta):
#w,b随机，eta为固定值
    for i in range(reader.num_train):
#reader.num_train为reader的函数
    x = X[i]
    y = Y[i]
    z = x*w+b
    dz = z - y
    dw = dz*x
    w = w - eta*dw
    b = b - eta*dz
#公式还原
```
# 神经网络法

**要求**：`初始化权重值，根据权重值放出一个解，根据损失函数求误差误差反向传播给线性计算部分以调整权重值是否满足终止条件？不满足的话跳回2`

**原理**：运用了神经元的结构，其原理与梯度下降法相似

```python
#神经元定义
class NeuralNet(object):
    def __init__(self, eta):
        self.eta = eta
        self.w = 0
        self.b = 0
```
# 多样本形式

**原理**：`多样本计算，就是把每次循环，所计算的一个数值变成一个矩阵的形式`

**要求**：在最小二乘法和神经网络法中均适用

$$x=x_1  -> 
X=\begin{pmatrix}
    x_1 \\\\ 
    x_2 \\\\ 
    x_3
\end{pmatrix}
$$

# 梯度训练形式

## 单次

优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。

## 小批量

优点：不受单样本噪声影响，训练速度较快。
缺点：每批量的数值选择很关键，会影响训练结果。

## 全部

优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。

# 一般框架

- 1.定义神经元结构
  
    - 神经元的输入和输出的方式和个数
    - 神经元的偏置和激活函数的选择

- 2.定义神经网络

    - 输入层 `输入的特征数`
    - 隐层 `数目可多个`
    - 输出层 `类别数目)`

- 3.损失函数定义
    
    - 选择与问题相对应的损失函数

        **如**：`线性回归问题使用均方差公式，二分类使用交叉熵公式`

# 数据标准化

**原理**：`计算机计算每条数据的时候，数据的大小可能过大或发散，或者在运算过程中造成数值问题，有可能导致学习力选择不太理想。`

**常用方法**：样本特征值和样本标签值都标准化，但预测值需要反标准化。

**非常用方法所带来的问题**：如果就只有特征值标准化，有两种方法，可以使预测值准确：

- 还原参数法，
- 将预测指标准化

**可选标准化方法**：

- Min-Max标准化（离差标准化），将数据映射到 $[0,1]$ 区间

$$x_{new}=\frac{x-x_{min}}{x_{max} - x_{min}} \tag{1}$$

- 平均值标准化，将数据映射到[-1,1]区间
   
$$x_{new} = \frac{x - \bar{x}}{x_{max} - x_{min}} \tag{2}$$

- 对数转换
$$x_{new}=\ln(x_i) \tag{3}$$

- 反正切转换
$$x_{new}=\frac{2}{\pi}\arctan(x_i) \tag{4}$$

- Z-Score法

把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。Z-Score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：

$$x_{new} = \frac{x_i - \bar{x}}{std} \tag{5}$$

- 中心化，平均值为0，无标准差要求
  
$$x_{new} = x_i - \bar{x} \tag{6}$$

- 比例法，要求数据全是正值

$$
x_{new} = \frac{x_k}{\sum_{i=1}^m{x_i}} \tag{7}
$$



