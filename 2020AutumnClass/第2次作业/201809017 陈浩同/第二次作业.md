## 线性回归
单层的神经网络，其实就是一个神经元，可以完成一些线性的工作，比如拟合一条直线，这用一个神经元就可以实现。当这个神经元只接收一个输入时，就是单变量线性回归，可以在二维平面上用可视化方法理解。当接收多个变量输入时，叫做多变量线性回归。
### 一元线性回归模型
$ Y=a+bX+\varepsilon $
回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。
$a$ 和 $b$ 是参数，在线性回归模型中，$a,b$ 是我们要通过算法学习出来的。

![](2021-01-17-11-24-55.png)
$$线性回归和非线性回归的区别$$
  线性回归问题的解决方法：
- 最小二乘法
- 梯度下降法
- 简单的神经网络法
- 更通用的神经网络法
  
### 最小二乘法
最小二乘法，也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最小二乘法来表达。

线性回归试图学得：
$z_i=w \cdot x_i+b $
使得：
$z_i \simeq y_i $
其中，$x_i$ 是样本特征值，$y_i$ 是样本标签值，$z_i$ 是模型预测值。

### 梯度下降法
梯度下降（gradient descent）在机器学习中应用十分的广泛，不论是在线性回归还是Logistic回归中，它的主要目的是通过迭代找到目标函数的最小值，或者收敛到最小值。

### 神经网络法
逻辑性的思维是指根据逻辑规则进行推理的过程；它先将信息化成概念，并用符号表示，然后，根据符号运算按串行模式进行逻辑推理；这一过程可以写成串行的指令，让计算机执行。然而，直观性的思维是将分布式存储的信息综合起来，结果是忽然间产生的想法或解决问题的办法。这种思维方式的根本之点在于以下两点：1.信息是通过神经元上的兴奋模式分布存储在网络上；2.信息处理是通过神经元之间同时相互作用的动态过程来完成的。
### 梯度下降的三种形式
- 单样本随机梯度下降
  优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
  缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。
- 小批量样本梯度下降
  优点：不受单样本噪声影响，训练速度较快。
  缺点：batch size的数值选择很关键，会影响训练结果。
- 全批量样本梯度下降
  优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
  缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。

### 正规方程解法
对于线性回归问题，除了前面提到的最小二乘法可以解决一元线性回归的问题外，也可以解决多元线性回归问题。

对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。
![](2020-10-06-23-38-40.png)


## 线性分类
分类问题在很多资料中都称之为逻辑回归，Logistic Regression，其原因是使用了线性回归中的线性模型，加上一个Logistic二分类函数，共同构造了一个分类器。我们在本书中统称之为分类。

神经网络的一个重要功能就是分类，现实世界中的分类任务复杂多样，但万变不离其宗，我们都可以用同一种模式的神经网络来处理。

### 逻辑回归模型

逻辑回归（Logistic Regression），回归给出的结果是事件成功或失败的概率。当因变量的类型属于二值（1/0，真/假，是/否）变量时，我们就应该使用逻辑回归。

### 二分类函数
对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。而在很多不太正规的文字材料中，把这两个概念混用了，比如下面这个说法：“我们在最后使用Sigmoid激活函数来做二分类”，这是不恰当的。在本书中，我们会根据不同的任务区分激活函数和分类函数这两个概念，在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。
![](2021-01-17-11-28-52.png)
### 多分类函数 
它有两个特点：
- 三个类别的概率相加为1
- 每个类别的概率都大于0


### 线性多分类实现
- 输入层
输入经度 $x_1$ 和纬度 $x_2$ 两个特征：
$$
x=\begin{pmatrix}
x_1 & x_2
\end{pmatrix}
$$
- 权重矩阵
$W$权重矩阵的尺寸，可以从前往后看，比如：输入层是2个特征，输出层是3个神经元，则$W$的尺寸就是 $2\times 3$。
$$
W=\begin{pmatrix}
w_{11} & w_{12} & w_{13}\\\\
w_{21} & w_{22} & w_{23} 
\end{pmatrix}
$$
$B$的尺寸是1x3，列数永远和神经元的数量一样，行数永远是1。

$$
B=\begin{pmatrix}
b_1 & b_2 & b_3 
\end{pmatrix}
$$

- 输出层
输出层三个神经元，再加上一个Softmax计算，最后有$A1,A2,A3$三个输出，写作：

$$
Z = \begin{pmatrix}z_1 & z_2 & z_3 \end{pmatrix}
$$
$$
A = \begin{pmatrix}a_1 & a_2 & a_3 \end{pmatrix}
$$

其中，$Z=X \cdot W+B，A = Softmax(Z)$

## 线性多分类
### 先行多分类过程
1. 线性计算

$z_1 = x_1 w_{11} + x_2 w_{21} + b_1 $
$z_2 = x_1 w_{12} + x_2 w_{22} + b_2 $
$z_3 = x_1 w_{13} + x_2 w_{23} + b_3 $

2. 分类计算

$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} 
$
$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}} 
$
$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}} 
$

3. 损失函数计算

单样本时，$n$表示类别数，$j$表示类别序号：

$
\begin{aligned}
loss(w,b)&=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3) \\\\
&=-\sum_{j=1}^{n} y_j \ln a_j 
\end{aligned}
$
批量样本时，$m$ 表示样本数，$i$ 表示样本序号：

$
\begin{aligned}
J(w,b) &=- \sum_{i=1}^m (y_{i1} \ln a_{i1} + y_{i2} \ln a_{i2} + y_{i3} \ln a_{i3}) \\\\
&=- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij}
\end{aligned}
$


### 心得体会：
通过本次对Step2和Step3的学习，初步了解了线性回归和线性分类，并对最小二乘法在数值计算中的应用进行了学习，梯度下降法是通过数学的计算，区别在于，最小二乘法从损失函数求导，直接求得数学解析解，而梯度下降以及后面的神经网络，都是利用导数传递误差，再通过迭代方式一步一步（用近似解）逼近真实解。
