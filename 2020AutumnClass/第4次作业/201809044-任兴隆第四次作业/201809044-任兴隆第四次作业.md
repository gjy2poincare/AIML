##深度神经网络搭建
###1，神经网络的基本构架
####1.1 神经网络
神经网络可以看作为一个复合函数，当你输入一些数据时，它便对应输出相应的数据。首先我们来看神经元的结构 1.1.1，神经元 将一个神经元看做一个函数，里面包含了函数所需要的数据和操作，以函数y=ax+b来说这个神经元，数据首先输入到这个神经元然后将权重a与数据x相乘，然后再向相乘的数据添加偏置项b,最后从另一端输出.然后再通过激活函数将数据x的输出限制在一个范围内.总的来说就是从一个数据点传入数据，计算多个输出值，再传递到下一个单元，这样一系列的操作便组成了一个神经网络。

1.1.2，神经网络中的数据 连接是神经网络必须学习的数值. 权重代表神经网络认为权重与数据相乘之后应该添加的内容. 超参数是改变机器行为的一系列数据就是超参数，超参数需要手动设置. 偏置项：有利于神经网络的收敛和数据的精确度.

1.2 深度神经网络的模块划分
批量输入模块 各种深度学习零件搭建的深度学习网络 优化模块

1.3 深度神经网络常用层
1.3.1，dense层，activation层，dropout层， flatten层

1.3.2 卷积层 卷积就是一种把相对应位置数据相乘然后再相加的运算，卷积神经网络可以用更少的参数对数据进行计算，得到最终的数据处理结果，卷积神经网络是由输入层，卷积层，激活函数，池化层，全连接层组成。

1.3.3 池化层 池化层可以非常有效的缩小参数矩阵的尺寸，从而减少最后dense层中的参数数量，加快计算速度，防止过拟合。

1.3.4 正则化层 正则化就是规则化，为给需要训练的目标函数加上一些限制，让它们不要超过一定的范围，也是用来防止过拟合。

1.3.5 反卷积层 卷积层通过矩阵来理解 正向和反向分别乘以c和ct。反卷积就是正向和反向分别乘以ct和c反卷积就是将输入范围变大在实际情况中，输出的范围是大于输入的范围的要想还原输入，实际上是无法还原的，但是通过反卷积可以最大程度上进行模拟学习的得到。

2，回归任务功能测试
2.1，搭建模型
模型如下所示：一个双层的神经网络，第一层后面接一个Sigmoid激活函数，第二层直接输出拟合数据

ch1

代码如下：

def model():
    dataReader = LoadData()(进行数据的读取)
    num_input = 1
    num_hidden1 = 4
    num_output = 1

    max_epoch = 10000
    batch_size = 10
    learning_rate = 0.5

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.Fitting,（调用fittingh函数）
        init_method=InitialMethod.Xavier,(调用xavier函数)
        stopper=Stopper(StopCondition.StopLoss, 0.001))

    net = NeuralNet_4_0(params, "Level1_CurveFittingNet")
    (调用param参数)
    fc1 = FcLayer_1_0(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    sigmoid1 = ActivationLayer(Sigmoid())(调用激活函数)
    net.add_layer(sigmoid1, "sigmoid1")(调用sigmoid参数)
    fc2 = FcLayer_1_0(num_hidden1, num_output, params)
    net.add_layer(fc2, "fc2")(调用fc2参数)

    net.train(dataReader, checkpoint=100, need_test=True)(神经网络样本训练)

    net.ShowLossHistory()
    ShowResult(net, dataReader)
超参数说明：

输入层1个神经元，因为只有一个x值
隐层4个神经元，对于此问题来说应该是足够了，因为特征很少
输出层1个神经元，因为是拟合任务
学习率=0.5
最大epoch=10000轮
批量样本数=10
拟合网络类型
Xavier初始化
绝对损失停止条件=0.001
2.2，训练结果
ch2 由上面的图可以看出：损失函数值在一段平缓期过后，开始陡降，这种现象在神经网络的训练中是常见的，最有可能的是当时处于一个梯度变化的平缓地带，算法在艰难地寻找下坡路，然后忽然就找到了。

ch3 上图左侧子图是拟合的情况，绿色点是测试集数据，红色点是神经网路的推理结果，可以看到除了最左侧开始的部分，其它部分都拟合相对较好。

右侧的子图是用下面的代码生成的：

    y_test_real = net.inference(dr.XTest)
    axes.scatter(y_test_real, y_test_real-dr.YTestRaw, marker='o')
以测试集的真实值为横坐标，以真实值和预测值的差为纵坐标。最理想的情况是所有点都在y=0处排成一条横线。从图上看，真实值和预测值二者的差异明显，但是请注意横坐标和纵坐标的间距相差一个数量级，所以差距其实不大。

再看打印输出的最后部分：

epoch=4999, total_iteration=449999
loss_train=0.000920, accuracy_train=0.968329
loss_valid=0.000839, accuracy_valid=0.962375
time used: 28.002626419067383
save parameters
total weights abs sum= 45.27530164993504
total weights = 8
little weights = 0
zero weights = 0
testing...
0.9817814550687021
0.9817814550687021
由于我们设置了eps=0.001，所以在5000多个epoch时便达到了要求，训练停止。最后用测试集得到的准确率为98.17%，已经非常不错了。如果训练更多的轮，可以得到更好的结果。

3 二分类任务功能测试
3.1 搭建模型
二分类任务模型同样是一个双层神经网络，但是最后一层要接一个Logistic二分类函数来完成二分类任务

ch4

代码如下：

def model(dataReader):
    num_input = 2
    num_hidden = 3
    num_output = 1

    max_epoch = 1000
    batch_size = 5
    learning_rate = 0.1

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.BinaryClassifier,(调用binaryclassifier函数)
        init_method=InitialMethod.Xavier,(调用xiavier函数)
        stopper=Stopper(StopCondition.StopLoss, 0.02))

    net = NeuralNet_4_0(params, "Arc")

    fc1 = FcLayer_1_0(num_input, num_hidden, params)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    sigmoid1 = ActivationLayer(Sigmoid())(调用sigmoid1激活函数)
    net.add_layer(sigmoid1, "sigmoid1")(调用sigmoid1参数)
    fc2 = FcLayer_1_0(num_hidden, num_output, params)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    logistic = ClassificationLayer(Logistic())(调用logistic函数)
    net.add_layer(logistic, "logistic")(调用logistic参数)

    net.train(dataReader, checkpoint=10, need_test=True)(神经网络样本训练)
    return net
超参数说明：

输入层神经元数为2
隐层的神经元数为3，使用Sigmoid激活函数
由于是二分类任务，所以输出层只有一个神经元，用Logistic做二分类函数
最多训练1000轮
批大小=5
学习率=0.1
绝对误差停止条件=0.02

epoch=419, total_iteration=30239
loss_train=0.010094, accuracy_train=1.000000
loss_valid=0.019141, accuracy_valid=1.000000
time used: 2.149379253387451
testing...
1.0
最后的testing...的结果是1.0，表示100%正确，这初步说明mini框架在这个基本case上工作得很好。下图分类结果也做的不错。

ch5

4. 多分类功能测试
在第11章里，我们讲解了如何使用神经网络做多分类。在本节我们将会用Mini框架重现那个教学案例，然后使用一个真实的案例验证多分类的用法。

4.1 搭建模型一
使用Sigmoid做为激活函数的两层网络 ch9

代码如下：

def model_sigmoid(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_sigmoid")(调用hp参数)

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    s1 = ActivationLayer(Sigmoid())
    net.add_layer(s1, "Sigmoid1")(调用s1参数)

    fc2 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    softmax1 = ClassificationLayer(Softmax())
    net.add_layer(softmax1, "softmax1")(调用softmax1参数)

    net.train(dataReader, checkpoint=50, need_test=True)(神经网络样本训练)
    net.ShowLossHistory()
    ShowResult(net, hp.toString())
    ShowData(dataReader)
超参数说明

隐层8个神经元
最大epoch=5000
批大小=10
学习率0.1
绝对误差停止条件=0.08
多分类网络类型
初始化方法为Xavier
net.train()函数是一个阻塞函数，只有当训练完毕后才返回。


5.1 搭建模型二
使用ReLU做为激活函数的三层网络 ch8

用两层网络也可以实现，但是使用ReLE函数时，训练效果不是很稳定，用三层比较保险。

代码如下

def model_relu(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_relu")(调用hp参数)

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "Relu1")(调用r1参数)

    fc2 = FcLayer_1_0(num_hidden, num_hidden, hp)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    r2 = ActivationLayer(Relu())
    net.add_layer(r2, "Relu2")(调用r2参数)

    fc3 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc3, "fc3")(调用fc3参数)
    softmax = ClassificationLayer(Softmax())(调用softmax函数)
    net.add_layer(softmax, "softmax")(调用softmax参数)

    net.train(dataReader, checkpoint=50, need_test=True)(神经网络样本训练)
    net.ShowLossHistory()
    ShowResult(net, hp.toString())
    ShowData(dataReader)
超参数说明

隐层8个神经元
最大epoch=5000
批大小=10
学习率0.1
绝对误差停止条件=0.08
多分类网络类型
初始化方法为MSRA
5.2运行结果


5.3 比较
下图是使用不同的激活函数的分类效果图和使用不同的激活函数的分类结果的比较。

Sigmoid	ReLU
ch11	ch12
从图片中可以看到左图中的边界要平滑许多，这也就是ReLU和Sigmoid的区别。ReLU是用分段线性拟合曲线，Sigmoid有真正的曲线拟合能力。但是Sigmoid也有缺点，看分类的边界，使用ReLU函数的分类边界比较清晰，而使用Sigmoid函数的分类边界要平缓一些，过渡区较宽。 Relu能直则直，对方形边界适用；Sigmoid能弯则弯，对圆形边界适用。

6 权重矩阵初始化
6.1 零初始化
首先我们就要谈到神经网络的权重初始化方法对模型的收敛速度和性能有着至关重要的影响。神经网络其实就是对权重参数w的不停迭代更新，以期达到较好的性能。在深度神经网络中，随着层数的增多，我们在梯度下降的过程中，极易出现梯度消失或者梯度爆炸。因此，对权重的初始化则显得至关重要，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸的问题，但是对于处理这两个问题是有很大的帮助的，并且十分有利于模型性能和收敛速度。

零初始化即把神经网络所有层的权重W值的初始值都设置为0。

$$ W = 0 $$

但是对于多层网络来说，绝对不能用零初始化，否则权重值不能学习到合理的结果。看下面的零值初始化的权重矩阵值打印输出：

W1= [[-0.82452497 -0.82452497 -0.82452497]]
B1= [[-0.01143752 -0.01143752 -0.01143752]]
W2= [[-0.68583865]
 [-0.68583865]
 [-0.68583865]]
B2= [[0.68359678]]
可以看到W1、B1、W2内部3个单元的值都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有W的值都同步更新，没有差别。这样的话，无论多少轮，最终的结果也不会正确。 下面还有几种初始化方法在此就不过多的叙述就看一下运行图片和代码吧！

6.2 标准初始化
基于激活函数sigmoid ch13

6.3 Xavier初始化
基于激活函数sigmoid ch14

基于激活函数relu ch16

6.4 MSRA初始化
基于激活函数relu ch15

代码如下：

def net(init_method, activator):

    max_epoch = 1
    batch_size = 5
    learning_rate = 0.02

    params = HyperParameters_4_1(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.Fitting,
        init_method=init_method)

    net = NeuralNet_4_1(params, "level1")
    num_hidden = [128,128,128,128,128,128,128](样本矩阵)
    fc_count = len(num_hidden)-1(样本矩阵的长度)
    layers = []

    for i in range(fc_count):
        fc = FcLayer_1_1(num_hidden[i], num_hidden[i+1], params)
        net.add_layer(fc, "fc")(调用fc参数)
        layers.append(fc)

        ac = ActivationLayer(activator)
        net.add_layer(ac, "activator")
        layers.append(ac)
    # end for
    # 从正态分布中取1000个样本，每个样本有num_hidden[0]个特征值
    # 转置是为了可以和w1做矩阵乘法
    x = np.random.randn(1000, num_hidden[0])

    # 激活函数输出值矩阵列表
    a_value = []

    # 依次做所有层的前向计算
    input = x
    for i in range(len(layers)):
        output = layers[i].forward(input)
        # 但是只记录激活层的输出
        if isinstance(layers[i], ActivationLayer):
            a_value.append(output)
        # end if
        input = output
    # end for

    for i in range(len(a_value)):
        ax = plt.subplot(1, fc_count+1, i+1)
        ax.set_title("layer" + str(i+1))
        plt.ylim(0,10000)
        if i > 0:
            plt.yticks([])
        ax.hist(a_value[i].flatten(), bins=25, range=[0,1])
    #end for
    # super title
    plt.suptitle(init_method.name + " : " + activator.get_name())
    plt.show()

if __name__ == '__main__':
    net(InitialMethod.Normal, Sigmoid())
    net(InitialMethod.Xavier, Sigmoid())
    net(InitialMethod.Xavier, Relu())
    net(InitialMethod.MSRA, Relu())

假设：

W参数服从高斯分布，即：$w_j \sim N(0,\tau^2)$
Y服从高斯分布，即：$y_i \sim N(w^Tx_i,\sigma^2)$
贝叶斯最大后验估计：

$$ \arg\max_wL(w) = \ln \prod_i^n \frac{1}{\sigma\sqrt{2 \pi}}\exp(-(\frac{y_i-w^Tx_i}{\sigma})^2/2) \cdot \prod_j^m{\frac{1}{\tau\sqrt{2\pi}}\exp(-(\frac{w_j}{\tau})^2/2)} $$

$$ =-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)^2-\frac{1}{2\tau^2}\sum_j^m{w_j^2}-n\ln\sigma\sqrt{2\pi}-m\ln \tau\sqrt{2\pi} \tag{3} $$

因为$\sigma,b,n,\pi,m$等都是常数，所以损失函数$J(w)$的最小值可以简化为：

$$ \arg\min_wJ(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{w_j^2} \tag{4} $$

看公式4，相当于是线性回归的均方差损失函数，再加上一个正则项，共同构成损失函数。如果想求这个函数的最小值，则需要两者协调，并不是说分别求其最小值就能实现整体最小，因为它们具有共同的W项，当W比较大时，第一项比较小，第二项比较大，或者正好相反。所以它们是矛盾组合体。

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项最小，可以使得W的每个元素都很小，都接近于0，因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是“抗扰动能力强”。

代码如下：

if __name__ == '__main__':
    dr = LoadData()
    hp, num_hidden = SetParameters()
    hp.regular_name = RegularMethod.L2
    hp.regular_value = 0.01
    net = Model(dr, 1, num_hidden, 1, hp)
    ShowResult(net, dr, hp.toString())

    10.2 L1正则化
L1正则化是在机器学习的Loss函数中，通常会添加一些正则化（正则化与一些贝叶斯先验本质上是一致的，L2正则化与高斯先验是一致的、L1正则化与拉普拉斯先验是一致）来降低模型的结构风险，这样可以使降低模型复杂度、防止参数过大 假设：

W参数服从拉普拉斯分布，即$w_j \sim Laplace(0,b)$
Y服从高斯分布，即$y_i \sim N(w^Tx_i,\sigma^2)$
贝叶斯最大后验估计： $$ \begin{aligned} \arg\max_wL(w) = &\ln \prod_i^n \frac{1}{\sigma\sqrt{2 \pi}}\exp(-\frac{1}{2}(\frac{y_i-w^Tx_i}{\sigma})^2) \cdot \prod_j^m{\frac{1}{2b}\exp(-\frac{\lvert w_j \rvert}{b})} \\ =&-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)^2-\frac{1}{2b}\sum_j^m{\lvert w_j \rvert} -n\ln\sigma\sqrt{2\pi}-m\ln b\sqrt{2\pi} \end{aligned} \tag{1} $$

因为$\sigma,b,n,\pi,m$等都是常数，所以损失函数$J(w)$的最小值可以简化为：

$$ \arg\min_wJ(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{\lvert w_j \rvert} \tag{2} $$

我们仍以两个参数为例，公式2的后半部分的正则形式为：

$$L_1 = \lvert w_1 \rvert + \lvert w_2 \rvert \tag{3}$$

因为$w_1,w_2$有可能是正数或者负数，我们令$x=|w_1|,y=|w_2|,c=L_1$，则公式3可以拆成以下4个公式的组合：

$$ y=-x+c \quad (当w_1 \gt 0, w_2 \gt 0时) $$ $$ y=\quad x+c \quad (当w_1 \lt 0, w_2 \gt 0时) $$ $$ y=\quad x-c \quad (当w_1 \gt 0, w_2 \lt 0时) $$ $$ y=-x-c \quad (当w_1 \lt 0, w_2 \lt 0时) $$

代码如下：

if __name__ == '__main__':
    dr = LoadData()
    hp, num_hidden = SetParameters()
    hp.regular_name = RegularMethod.L1
    hp.regular_value = 0.005
    net = Model(dr, 1, num_hidden, 1, hp)
    ShowResult(net, dr, hp.toString())
运行结果如下图所示: ch33 ch34 ch35

本章总结：
在深度学习中，对神经网络的权重进行初始化对模型的收敛速度和性能的提升有着重要的影响。而且在神经网络在计算过程中需要对权重参数w不断的迭代更新，已达到较好的性能效果。但在训练的过程中，会遇到梯度消失和梯度爆炸等现象。因此，一个好的初始化权重能够对这两个问题有很好的帮助，并且，初始化权重能够有利于模型性能的提升，以及增快收敛速度。

梯度下降优化算法 1,BGD 优点： 对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。 缺点： 速度慢; 数据量大时不可行; 无法处理动态产生的新样本。 2,SGD 优点： 更新频次快，优化速度更快; 可以无法处理动态产生的新样本；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度) 缺点： 随机性可能导致收敛复杂化，即使到达最优点仍然会进行过度优化，因此SGD得优化过程相比BGD充满动荡；

对于批量归一化：如果激活函数是类似于sigmoid有一定的饱和区域则可以把归一化层放在激活函数之前，这样可以在一定程度上缓解梯度消失问题。 如果激活函数是类似于relu这样的激活函数，那么可以把归一化层放在激活函数之后，可以有效避免数据在激活之前被转化成相似的模式从而使得非线性特征分布趋于同化。

集成学习的主要思路是先通过一定的规则生成多个学习器，再采用某种集成策略进行组合，最后综合判断输出最终结果。一般而言，通常所说的集成学习中的多个学习器都是同质的"弱学习器"。基于该弱学习器，通过样本集扰动、输入特征扰动、输出表示扰动、算法参数扰动等方式生成多个学习器，进行集成后获得一个精度较好的"强学习器"。

防止过拟合正则化的本质：约束要优化的参数 L1正则化在实际中往往替代L0正则化，来防止过拟合。 L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。 